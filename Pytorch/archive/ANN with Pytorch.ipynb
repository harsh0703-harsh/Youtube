{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"abalone_original.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>length</th>\n",
       "      <th>diameter</th>\n",
       "      <th>height</th>\n",
       "      <th>whole-weight</th>\n",
       "      <th>shucked-weight</th>\n",
       "      <th>viscera-weight</th>\n",
       "      <th>shell-weight</th>\n",
       "      <th>rings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>91</td>\n",
       "      <td>73</td>\n",
       "      <td>19</td>\n",
       "      <td>102.8</td>\n",
       "      <td>44.9</td>\n",
       "      <td>20.2</td>\n",
       "      <td>30.0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>70</td>\n",
       "      <td>53</td>\n",
       "      <td>18</td>\n",
       "      <td>45.1</td>\n",
       "      <td>19.9</td>\n",
       "      <td>9.7</td>\n",
       "      <td>14.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F</td>\n",
       "      <td>106</td>\n",
       "      <td>84</td>\n",
       "      <td>27</td>\n",
       "      <td>135.4</td>\n",
       "      <td>51.3</td>\n",
       "      <td>28.3</td>\n",
       "      <td>42.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>88</td>\n",
       "      <td>73</td>\n",
       "      <td>25</td>\n",
       "      <td>103.2</td>\n",
       "      <td>43.1</td>\n",
       "      <td>22.8</td>\n",
       "      <td>31.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I</td>\n",
       "      <td>66</td>\n",
       "      <td>51</td>\n",
       "      <td>16</td>\n",
       "      <td>41.0</td>\n",
       "      <td>17.9</td>\n",
       "      <td>7.9</td>\n",
       "      <td>11.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4172</th>\n",
       "      <td>F</td>\n",
       "      <td>113</td>\n",
       "      <td>90</td>\n",
       "      <td>33</td>\n",
       "      <td>177.4</td>\n",
       "      <td>74.0</td>\n",
       "      <td>47.8</td>\n",
       "      <td>49.8</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4173</th>\n",
       "      <td>M</td>\n",
       "      <td>118</td>\n",
       "      <td>88</td>\n",
       "      <td>27</td>\n",
       "      <td>193.2</td>\n",
       "      <td>87.8</td>\n",
       "      <td>42.9</td>\n",
       "      <td>52.1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4174</th>\n",
       "      <td>M</td>\n",
       "      <td>120</td>\n",
       "      <td>95</td>\n",
       "      <td>41</td>\n",
       "      <td>235.2</td>\n",
       "      <td>105.1</td>\n",
       "      <td>57.5</td>\n",
       "      <td>61.6</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4175</th>\n",
       "      <td>F</td>\n",
       "      <td>125</td>\n",
       "      <td>97</td>\n",
       "      <td>30</td>\n",
       "      <td>218.9</td>\n",
       "      <td>106.2</td>\n",
       "      <td>52.2</td>\n",
       "      <td>59.2</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4176</th>\n",
       "      <td>M</td>\n",
       "      <td>142</td>\n",
       "      <td>111</td>\n",
       "      <td>39</td>\n",
       "      <td>389.7</td>\n",
       "      <td>189.1</td>\n",
       "      <td>75.3</td>\n",
       "      <td>99.0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4177 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sex  length  diameter  height  whole-weight  shucked-weight  \\\n",
       "0      M      91        73      19         102.8            44.9   \n",
       "1      M      70        53      18          45.1            19.9   \n",
       "2      F     106        84      27         135.4            51.3   \n",
       "3      M      88        73      25         103.2            43.1   \n",
       "4      I      66        51      16          41.0            17.9   \n",
       "...   ..     ...       ...     ...           ...             ...   \n",
       "4172   F     113        90      33         177.4            74.0   \n",
       "4173   M     118        88      27         193.2            87.8   \n",
       "4174   M     120        95      41         235.2           105.1   \n",
       "4175   F     125        97      30         218.9           106.2   \n",
       "4176   M     142       111      39         389.7           189.1   \n",
       "\n",
       "      viscera-weight  shell-weight  rings  \n",
       "0               20.2          30.0     15  \n",
       "1                9.7          14.0      7  \n",
       "2               28.3          42.0      9  \n",
       "3               22.8          31.0     10  \n",
       "4                7.9          11.0      7  \n",
       "...              ...           ...    ...  \n",
       "4172            47.8          49.8     11  \n",
       "4173            42.9          52.1     10  \n",
       "4174            57.5          61.6      9  \n",
       "4175            52.2          59.2     10  \n",
       "4176            75.3          99.0     12  \n",
       "\n",
       "[4177 rows x 9 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sex']=pd.get_dummies(data['sex'],drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>length</th>\n",
       "      <th>diameter</th>\n",
       "      <th>height</th>\n",
       "      <th>whole-weight</th>\n",
       "      <th>shucked-weight</th>\n",
       "      <th>viscera-weight</th>\n",
       "      <th>shell-weight</th>\n",
       "      <th>rings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>91</td>\n",
       "      <td>73</td>\n",
       "      <td>19</td>\n",
       "      <td>102.8</td>\n",
       "      <td>44.9</td>\n",
       "      <td>20.2</td>\n",
       "      <td>30.0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "      <td>53</td>\n",
       "      <td>18</td>\n",
       "      <td>45.1</td>\n",
       "      <td>19.9</td>\n",
       "      <td>9.7</td>\n",
       "      <td>14.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>106</td>\n",
       "      <td>84</td>\n",
       "      <td>27</td>\n",
       "      <td>135.4</td>\n",
       "      <td>51.3</td>\n",
       "      <td>28.3</td>\n",
       "      <td>42.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>88</td>\n",
       "      <td>73</td>\n",
       "      <td>25</td>\n",
       "      <td>103.2</td>\n",
       "      <td>43.1</td>\n",
       "      <td>22.8</td>\n",
       "      <td>31.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>66</td>\n",
       "      <td>51</td>\n",
       "      <td>16</td>\n",
       "      <td>41.0</td>\n",
       "      <td>17.9</td>\n",
       "      <td>7.9</td>\n",
       "      <td>11.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4172</th>\n",
       "      <td>0</td>\n",
       "      <td>113</td>\n",
       "      <td>90</td>\n",
       "      <td>33</td>\n",
       "      <td>177.4</td>\n",
       "      <td>74.0</td>\n",
       "      <td>47.8</td>\n",
       "      <td>49.8</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4173</th>\n",
       "      <td>0</td>\n",
       "      <td>118</td>\n",
       "      <td>88</td>\n",
       "      <td>27</td>\n",
       "      <td>193.2</td>\n",
       "      <td>87.8</td>\n",
       "      <td>42.9</td>\n",
       "      <td>52.1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4174</th>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>95</td>\n",
       "      <td>41</td>\n",
       "      <td>235.2</td>\n",
       "      <td>105.1</td>\n",
       "      <td>57.5</td>\n",
       "      <td>61.6</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4175</th>\n",
       "      <td>0</td>\n",
       "      <td>125</td>\n",
       "      <td>97</td>\n",
       "      <td>30</td>\n",
       "      <td>218.9</td>\n",
       "      <td>106.2</td>\n",
       "      <td>52.2</td>\n",
       "      <td>59.2</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4176</th>\n",
       "      <td>0</td>\n",
       "      <td>142</td>\n",
       "      <td>111</td>\n",
       "      <td>39</td>\n",
       "      <td>389.7</td>\n",
       "      <td>189.1</td>\n",
       "      <td>75.3</td>\n",
       "      <td>99.0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4177 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sex  length  diameter  height  whole-weight  shucked-weight  \\\n",
       "0       0      91        73      19         102.8            44.9   \n",
       "1       0      70        53      18          45.1            19.9   \n",
       "2       0     106        84      27         135.4            51.3   \n",
       "3       0      88        73      25         103.2            43.1   \n",
       "4       1      66        51      16          41.0            17.9   \n",
       "...   ...     ...       ...     ...           ...             ...   \n",
       "4172    0     113        90      33         177.4            74.0   \n",
       "4173    0     118        88      27         193.2            87.8   \n",
       "4174    0     120        95      41         235.2           105.1   \n",
       "4175    0     125        97      30         218.9           106.2   \n",
       "4176    0     142       111      39         389.7           189.1   \n",
       "\n",
       "      viscera-weight  shell-weight  rings  \n",
       "0               20.2          30.0     15  \n",
       "1                9.7          14.0      7  \n",
       "2               28.3          42.0      9  \n",
       "3               22.8          31.0     10  \n",
       "4                7.9          11.0      7  \n",
       "...              ...           ...    ...  \n",
       "4172            47.8          49.8     11  \n",
       "4173            42.9          52.1     10  \n",
       "4174            57.5          61.6      9  \n",
       "4175            52.2          59.2     10  \n",
       "4176            75.3          99.0     12  \n",
       "\n",
       "[4177 rows x 9 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=data.iloc[:,0:8]\n",
    "y=data['rings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>length</th>\n",
       "      <th>diameter</th>\n",
       "      <th>height</th>\n",
       "      <th>whole-weight</th>\n",
       "      <th>shucked-weight</th>\n",
       "      <th>viscera-weight</th>\n",
       "      <th>shell-weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>91</td>\n",
       "      <td>73</td>\n",
       "      <td>19</td>\n",
       "      <td>102.8</td>\n",
       "      <td>44.9</td>\n",
       "      <td>20.2</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "      <td>53</td>\n",
       "      <td>18</td>\n",
       "      <td>45.1</td>\n",
       "      <td>19.9</td>\n",
       "      <td>9.7</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>106</td>\n",
       "      <td>84</td>\n",
       "      <td>27</td>\n",
       "      <td>135.4</td>\n",
       "      <td>51.3</td>\n",
       "      <td>28.3</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>88</td>\n",
       "      <td>73</td>\n",
       "      <td>25</td>\n",
       "      <td>103.2</td>\n",
       "      <td>43.1</td>\n",
       "      <td>22.8</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>66</td>\n",
       "      <td>51</td>\n",
       "      <td>16</td>\n",
       "      <td>41.0</td>\n",
       "      <td>17.9</td>\n",
       "      <td>7.9</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4172</th>\n",
       "      <td>0</td>\n",
       "      <td>113</td>\n",
       "      <td>90</td>\n",
       "      <td>33</td>\n",
       "      <td>177.4</td>\n",
       "      <td>74.0</td>\n",
       "      <td>47.8</td>\n",
       "      <td>49.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4173</th>\n",
       "      <td>0</td>\n",
       "      <td>118</td>\n",
       "      <td>88</td>\n",
       "      <td>27</td>\n",
       "      <td>193.2</td>\n",
       "      <td>87.8</td>\n",
       "      <td>42.9</td>\n",
       "      <td>52.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4174</th>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>95</td>\n",
       "      <td>41</td>\n",
       "      <td>235.2</td>\n",
       "      <td>105.1</td>\n",
       "      <td>57.5</td>\n",
       "      <td>61.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4175</th>\n",
       "      <td>0</td>\n",
       "      <td>125</td>\n",
       "      <td>97</td>\n",
       "      <td>30</td>\n",
       "      <td>218.9</td>\n",
       "      <td>106.2</td>\n",
       "      <td>52.2</td>\n",
       "      <td>59.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4176</th>\n",
       "      <td>0</td>\n",
       "      <td>142</td>\n",
       "      <td>111</td>\n",
       "      <td>39</td>\n",
       "      <td>389.7</td>\n",
       "      <td>189.1</td>\n",
       "      <td>75.3</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4177 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sex  length  diameter  height  whole-weight  shucked-weight  \\\n",
       "0       0      91        73      19         102.8            44.9   \n",
       "1       0      70        53      18          45.1            19.9   \n",
       "2       0     106        84      27         135.4            51.3   \n",
       "3       0      88        73      25         103.2            43.1   \n",
       "4       1      66        51      16          41.0            17.9   \n",
       "...   ...     ...       ...     ...           ...             ...   \n",
       "4172    0     113        90      33         177.4            74.0   \n",
       "4173    0     118        88      27         193.2            87.8   \n",
       "4174    0     120        95      41         235.2           105.1   \n",
       "4175    0     125        97      30         218.9           106.2   \n",
       "4176    0     142       111      39         389.7           189.1   \n",
       "\n",
       "      viscera-weight  shell-weight  \n",
       "0               20.2          30.0  \n",
       "1                9.7          14.0  \n",
       "2               28.3          42.0  \n",
       "3               22.8          31.0  \n",
       "4                7.9          11.0  \n",
       "...              ...           ...  \n",
       "4172            47.8          49.8  \n",
       "4173            42.9          52.1  \n",
       "4174            57.5          61.6  \n",
       "4175            52.2          59.2  \n",
       "4176            75.3          99.0  \n",
       "\n",
       "[4177 rows x 8 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[4175]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "668     13\n",
       "1580     8\n",
       "3784    11\n",
       "463      5\n",
       "2615    12\n",
       "        ..\n",
       "575     11\n",
       "3231    12\n",
       "1084     7\n",
       "290     17\n",
       "2713     4\n",
       "Name: rings, Length: 836, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3141     5\n",
       "3521     3\n",
       "883     15\n",
       "3627    10\n",
       "2106    14\n",
       "        ..\n",
       "1033    10\n",
       "3264    12\n",
       "1653    10\n",
       "2607     9\n",
       "2732     8\n",
       "Name: rings, Length: 3341, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we have to convert my data into tensors with Float data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=torch.FloatTensor(np.asarray(X_train))\n",
    "X_test=torch.FloatTensor(np.asarray(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.4761, -2.8886, -2.7757, -1.4109, -1.6309, -1.5576, -1.5894, -1.6550])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=torch.tensor(np.asarray(y_train))\n",
    "y_test=torch.tensor(np.asarray(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([13,  8, 11,  5, 12, 11,  7,  8,  7,  9,  8,  8, 11,  9,  4,  7,  7, 17,\n",
       "         7,  7,  7,  5,  8,  9, 10, 10,  5, 15, 10, 14,  8,  4,  9, 13,  7,  7,\n",
       "         8,  9,  8, 11, 15, 12, 17, 16, 11, 11,  8, 10, 11,  6, 13,  7, 13, 20,\n",
       "        12,  7,  8, 10,  7,  7,  9,  9, 11,  8,  7, 12, 13, 17,  8, 12,  9, 17,\n",
       "        10,  6, 11,  8,  8,  9,  8,  8,  8,  6,  7, 13, 11,  6,  9, 12,  5,  6,\n",
       "        11, 11,  8,  7, 16,  8, 11,  8, 18, 11, 12, 14, 12, 11,  6,  9,  7,  6,\n",
       "        11, 11, 11, 12, 20, 10, 14, 10, 10, 12,  4, 12,  7, 13,  6,  8, 17, 15,\n",
       "         9, 11,  7, 11,  8,  7,  7, 10, 11,  9, 10,  9,  8, 16, 16, 13,  6,  9,\n",
       "        10,  9,  8,  8,  8,  5,  8, 11,  5,  9,  9,  6,  8,  7, 10, 11, 12,  8,\n",
       "         9, 10,  5, 16,  7,  6, 15,  9, 10,  3,  6, 10, 11, 12,  5,  8,  5, 13,\n",
       "         9, 11,  7,  9, 15,  8, 10,  8,  5,  9, 19,  9,  8,  9, 11,  9,  9,  9,\n",
       "         5, 15, 10, 14, 12,  8, 12, 11,  5, 10, 11, 23,  4, 11, 11, 10,  9,  9,\n",
       "         9,  8,  5, 13, 12, 14,  8, 11, 10, 14, 13, 12, 12, 12,  7,  7,  7, 19,\n",
       "         9, 15,  8,  4,  8,  9, 10, 12,  9,  9,  9,  8, 12,  7, 11, 20,  8,  7,\n",
       "        12, 13, 11, 14,  6,  8,  9, 11,  9,  8,  9, 11, 20,  9, 12,  7,  6,  7,\n",
       "        11,  7,  9,  9,  5,  9,  9,  9, 10, 11,  7, 12,  8, 10, 12, 13,  9, 10,\n",
       "        11, 10,  8,  9,  4,  9,  7,  8,  6,  6,  9, 12, 10,  8, 15, 10, 13, 10,\n",
       "         8, 10, 14,  8,  9, 12,  8,  8, 12,  9, 10,  7,  8, 10, 11, 14, 10, 10,\n",
       "        15,  8,  9,  9, 23, 10,  8,  8,  5,  9, 10, 13,  7,  8, 11, 13,  9, 12,\n",
       "        11, 10, 10,  6,  5, 12, 15,  7,  5, 10, 10, 13, 10, 11, 10,  9,  7, 13,\n",
       "        10, 10,  7,  9, 13,  5,  6,  9,  7, 11, 11,  7, 14,  7, 17, 12, 12,  8,\n",
       "        12,  6,  8, 14, 11, 15,  5, 11, 11,  8,  7,  7, 14,  8, 14, 14, 21,  8,\n",
       "        11,  8, 11, 19,  8,  9,  7, 10,  9, 10,  5, 19, 16,  9, 10,  7, 11, 12,\n",
       "        15, 11, 20, 12, 13, 10,  8, 10, 11,  9,  5, 10, 11, 11, 12, 16,  9, 11,\n",
       "         9, 14, 14, 11, 11,  9, 12, 11,  7,  6, 12,  9,  7,  6, 10,  4, 15, 10,\n",
       "         3,  5,  9,  6,  5, 11, 11,  9,  8, 13,  7, 14,  7,  8, 12, 10, 11, 14,\n",
       "        16,  6, 10, 10,  8, 10,  8, 11,  7,  5,  9, 10, 12,  9,  8, 12, 16, 11,\n",
       "         9, 13,  8, 10,  3,  7, 10, 10,  6,  6,  9,  6, 13,  9,  7, 18,  5, 10,\n",
       "         9,  9,  9, 18,  9,  7,  9,  8, 18, 15, 10,  8, 12, 12, 15,  8,  8,  9,\n",
       "         9, 10,  6, 16, 10, 10,  7,  7,  8,  8,  9,  9,  5,  9, 10, 12, 10, 13,\n",
       "        10, 12,  8, 12,  5,  9,  8,  8, 10, 11, 10, 17, 12, 11,  8,  8,  8,  9,\n",
       "         9, 12, 13, 12, 10,  9, 13,  6, 11,  9, 15,  8, 13,  8,  8,  8,  9,  8,\n",
       "        10,  9, 11,  3,  8,  6,  6,  6,  9,  9, 17, 10,  8, 13,  7,  9, 14,  7,\n",
       "        13, 10,  4, 11,  7, 10,  8, 11, 10, 12, 10,  9,  7, 10, 10, 11,  6, 11,\n",
       "        12,  9, 13, 12, 12,  7, 13, 16, 11, 11,  6,  8, 12,  9,  7, 12, 10, 12,\n",
       "        13,  7, 12,  7, 11, 11,  5, 10,  8,  9, 14, 10, 12,  9,  9, 11,  6, 14,\n",
       "        13, 18,  7,  7, 10,  8,  7,  9,  9,  9, 10, 16, 14,  8, 20, 15, 13,  8,\n",
       "        10,  7,  6,  9, 10, 10, 12, 11,  5,  8,  8,  7,  7, 10, 15,  8, 10, 10,\n",
       "         9,  7,  9, 11,  9, 19, 10, 10,  9, 13,  7,  9, 29, 10,  8,  9,  6, 10,\n",
       "        10, 20,  9,  8, 11,  6, 11, 13,  6,  5,  7,  9,  8,  7, 14,  8,  7,  7,\n",
       "        20,  4, 10,  6, 13,  7, 11,  9, 19,  9,  7, 11, 10, 19,  8, 16,  7, 11,\n",
       "         7,  6, 12,  9, 10,  6, 12,  7,  9,  8,  7, 11,  8,  8,  8, 11, 10, 12,\n",
       "         7,  7,  8, 21,  7,  4,  7,  6, 12, 10,  9,  7,  9,  6,  7, 11, 11, 11,\n",
       "         8, 11,  6, 22, 10,  9, 17, 14,  7,  7, 16,  9,  8, 13, 10, 12,  8, 11,\n",
       "        12,  9, 14,  9, 11,  9,  3, 14, 10,  7,  7,  6,  7,  7,  9,  9,  7,  6,\n",
       "         9,  7, 10, 10, 10, 11, 10, 11, 19,  8,  5,  6, 11,  5,  8,  8,  9, 13,\n",
       "         5,  7,  9, 11, 12,  7, 17,  4])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(8,15)\n",
    "        self.fc2 = torch.nn.Linear(15,11)\n",
    "        self.fc3= torch.nn.Linear(11,12)\n",
    "        self.output=torch.nn.Linear(12,1)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x= F.relu(self.fc3(x))\n",
    "        x=self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Neural_Network=ANN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function=nn.L1Loss()\n",
    "optimizer=torch.optim.Adam(Neural_Network.parameters(),lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_loss=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\harsh\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\torch\\nn\\modules\\loss.py:94: UserWarning: Using a target size (torch.Size([3341])) that is different to the input size (torch.Size([3341, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Epoc  9.85359001159668 loss\n",
      "1 Epoc  9.765630722045898 loss\n",
      "2 Epoc  9.666389465332031 loss\n",
      "3 Epoc  9.551810264587402 loss\n",
      "4 Epoc  9.4102783203125 loss\n",
      "5 Epoc  9.231095314025879 loss\n",
      "6 Epoc  9.004941940307617 loss\n",
      "7 Epoc  8.72374153137207 loss\n",
      "8 Epoc  8.376784324645996 loss\n",
      "9 Epoc  7.951601982116699 loss\n",
      "10 Epoc  7.435754776000977 loss\n",
      "11 Epoc  6.82506799697876 loss\n",
      "12 Epoc  6.133891582489014 loss\n",
      "13 Epoc  5.412964820861816 loss\n",
      "14 Epoc  4.771329402923584 loss\n",
      "15 Epoc  4.365148067474365 loss\n",
      "16 Epoc  4.309399604797363 loss\n",
      "17 Epoc  4.54079532623291 loss\n",
      "18 Epoc  4.822929859161377 loss\n",
      "19 Epoc  4.958381175994873 loss\n",
      "20 Epoc  4.896511077880859 loss\n",
      "21 Epoc  4.679118633270264 loss\n",
      "22 Epoc  4.383902549743652 loss\n",
      "23 Epoc  4.086001396179199 loss\n",
      "24 Epoc  3.8452155590057373 loss\n",
      "25 Epoc  3.692563533782959 loss\n",
      "26 Epoc  3.624347686767578 loss\n",
      "27 Epoc  3.6111199855804443 loss\n",
      "28 Epoc  3.615459680557251 loss\n",
      "29 Epoc  3.6019201278686523 loss\n",
      "30 Epoc  3.549747943878174 loss\n",
      "31 Epoc  3.4530701637268066 loss\n",
      "32 Epoc  3.3191537857055664 loss\n",
      "33 Epoc  3.164933443069458 loss\n",
      "34 Epoc  3.0128562450408936 loss\n",
      "35 Epoc  2.8913686275482178 loss\n",
      "36 Epoc  2.8167524337768555 loss\n",
      "37 Epoc  2.7914721965789795 loss\n",
      "38 Epoc  2.792689561843872 loss\n",
      "39 Epoc  2.787994623184204 loss\n",
      "40 Epoc  2.7503204345703125 loss\n",
      "41 Epoc  2.678379535675049 loss\n",
      "42 Epoc  2.5909264087677 loss\n",
      "43 Epoc  2.5156664848327637 loss\n",
      "44 Epoc  2.4737648963928223 loss\n",
      "45 Epoc  2.4654901027679443 loss\n",
      "46 Epoc  2.4753177165985107 loss\n",
      "47 Epoc  2.4832093715667725 loss\n",
      "48 Epoc  2.4765214920043945 loss\n",
      "49 Epoc  2.4533255100250244 loss\n",
      "50 Epoc  2.422423839569092 loss\n",
      "51 Epoc  2.4010379314422607 loss\n",
      "52 Epoc  2.3955698013305664 loss\n",
      "53 Epoc  2.4005844593048096 loss\n",
      "54 Epoc  2.408923625946045 loss\n",
      "55 Epoc  2.4123642444610596 loss\n",
      "56 Epoc  2.4055593013763428 loss\n",
      "57 Epoc  2.3916544914245605 loss\n",
      "58 Epoc  2.378980875015259 loss\n",
      "59 Epoc  2.37428617477417 loss\n",
      "60 Epoc  2.3770318031311035 loss\n",
      "61 Epoc  2.3816587924957275 loss\n",
      "62 Epoc  2.3833041191101074 loss\n",
      "63 Epoc  2.3802030086517334 loss\n",
      "64 Epoc  2.3742873668670654 loss\n",
      "65 Epoc  2.369719982147217 loss\n",
      "66 Epoc  2.368312358856201 loss\n",
      "67 Epoc  2.368311882019043 loss\n",
      "68 Epoc  2.3664352893829346 loss\n",
      "69 Epoc  2.362372636795044 loss\n",
      "70 Epoc  2.357905626296997 loss\n",
      "71 Epoc  2.3548150062561035 loss\n",
      "72 Epoc  2.353853940963745 loss\n",
      "73 Epoc  2.353893518447876 loss\n",
      "74 Epoc  2.3536548614501953 loss\n",
      "75 Epoc  2.352938413619995 loss\n",
      "76 Epoc  2.352142095565796 loss\n",
      "77 Epoc  2.3515961170196533 loss\n",
      "78 Epoc  2.351410388946533 loss\n",
      "79 Epoc  2.3513290882110596 loss\n",
      "80 Epoc  2.351044178009033 loss\n",
      "81 Epoc  2.3504881858825684 loss\n",
      "82 Epoc  2.3497776985168457 loss\n",
      "83 Epoc  2.349175453186035 loss\n",
      "84 Epoc  2.3488235473632812 loss\n",
      "85 Epoc  2.348680257797241 loss\n",
      "86 Epoc  2.3486034870147705 loss\n",
      "87 Epoc  2.348435163497925 loss\n",
      "88 Epoc  2.3481221199035645 loss\n",
      "89 Epoc  2.347689628601074 loss\n",
      "90 Epoc  2.347252607345581 loss\n",
      "91 Epoc  2.34686541557312 loss\n",
      "92 Epoc  2.3465394973754883 loss\n",
      "93 Epoc  2.34622859954834 loss\n",
      "94 Epoc  2.3459556102752686 loss\n",
      "95 Epoc  2.345709800720215 loss\n",
      "96 Epoc  2.3454830646514893 loss\n",
      "97 Epoc  2.3452701568603516 loss\n",
      "98 Epoc  2.3450915813446045 loss\n",
      "99 Epoc  2.3449363708496094 loss\n",
      "100 Epoc  2.3448057174682617 loss\n",
      "101 Epoc  2.344679832458496 loss\n",
      "102 Epoc  2.344552755355835 loss\n",
      "103 Epoc  2.344423770904541 loss\n",
      "104 Epoc  2.344289779663086 loss\n",
      "105 Epoc  2.3441576957702637 loss\n",
      "106 Epoc  2.344032049179077 loss\n",
      "107 Epoc  2.343918800354004 loss\n",
      "108 Epoc  2.3438174724578857 loss\n",
      "109 Epoc  2.343736410140991 loss\n",
      "110 Epoc  2.3436689376831055 loss\n",
      "111 Epoc  2.3435959815979004 loss\n",
      "112 Epoc  2.3435020446777344 loss\n",
      "113 Epoc  2.343397378921509 loss\n",
      "114 Epoc  2.3432974815368652 loss\n",
      "115 Epoc  2.3432095050811768 loss\n",
      "116 Epoc  2.343129873275757 loss\n",
      "117 Epoc  2.3430540561676025 loss\n",
      "118 Epoc  2.342979907989502 loss\n",
      "119 Epoc  2.342905044555664 loss\n",
      "120 Epoc  2.342829704284668 loss\n",
      "121 Epoc  2.3427531719207764 loss\n",
      "122 Epoc  2.342674970626831 loss\n",
      "123 Epoc  2.342597246170044 loss\n",
      "124 Epoc  2.342522382736206 loss\n",
      "125 Epoc  2.3424508571624756 loss\n",
      "126 Epoc  2.342384099960327 loss\n",
      "127 Epoc  2.3423197269439697 loss\n",
      "128 Epoc  2.342254877090454 loss\n",
      "129 Epoc  2.3421878814697266 loss\n",
      "130 Epoc  2.34212064743042 loss\n",
      "131 Epoc  2.3420543670654297 loss\n",
      "132 Epoc  2.3419909477233887 loss\n",
      "133 Epoc  2.3419289588928223 loss\n",
      "134 Epoc  2.3418681621551514 loss\n",
      "135 Epoc  2.341808557510376 loss\n",
      "136 Epoc  2.3417537212371826 loss\n",
      "137 Epoc  2.3416969776153564 loss\n",
      "138 Epoc  2.3416383266448975 loss\n",
      "139 Epoc  2.341578483581543 loss\n",
      "140 Epoc  2.3415207862854004 loss\n",
      "141 Epoc  2.341468095779419 loss\n",
      "142 Epoc  2.3414173126220703 loss\n",
      "143 Epoc  2.3413665294647217 loss\n",
      "144 Epoc  2.3413143157958984 loss\n",
      "145 Epoc  2.3412625789642334 loss\n",
      "146 Epoc  2.3412115573883057 loss\n",
      "147 Epoc  2.3411622047424316 loss\n",
      "148 Epoc  2.3411154747009277 loss\n",
      "149 Epoc  2.341069459915161 loss\n",
      "150 Epoc  2.3410215377807617 loss\n",
      "151 Epoc  2.340973138809204 loss\n",
      "152 Epoc  2.3409252166748047 loss\n",
      "153 Epoc  2.340878486633301 loss\n",
      "154 Epoc  2.3408331871032715 loss\n",
      "155 Epoc  2.3407881259918213 loss\n",
      "156 Epoc  2.3407435417175293 loss\n",
      "157 Epoc  2.340698719024658 loss\n",
      "158 Epoc  2.3406565189361572 loss\n",
      "159 Epoc  2.3406126499176025 loss\n",
      "160 Epoc  2.3405675888061523 loss\n",
      "161 Epoc  2.3405256271362305 loss\n",
      "162 Epoc  2.3404839038848877 loss\n",
      "163 Epoc  2.340442180633545 loss\n",
      "164 Epoc  2.3404011726379395 loss\n",
      "165 Epoc  2.340360641479492 loss\n",
      "166 Epoc  2.3403196334838867 loss\n",
      "167 Epoc  2.3402795791625977 loss\n",
      "168 Epoc  2.3402395248413086 loss\n",
      "169 Epoc  2.3401997089385986 loss\n",
      "170 Epoc  2.3401598930358887 loss\n",
      "171 Epoc  2.340120315551758 loss\n",
      "172 Epoc  2.3400814533233643 loss\n",
      "173 Epoc  2.34004282951355 loss\n",
      "174 Epoc  2.3400039672851562 loss\n",
      "175 Epoc  2.339965581893921 loss\n",
      "176 Epoc  2.3399274349212646 loss\n",
      "177 Epoc  2.3398892879486084 loss\n",
      "178 Epoc  2.339850902557373 loss\n",
      "179 Epoc  2.339812755584717 loss\n",
      "180 Epoc  2.3397746086120605 loss\n",
      "181 Epoc  2.3397371768951416 loss\n",
      "182 Epoc  2.339700937271118 loss\n",
      "183 Epoc  2.3396658897399902 loss\n",
      "184 Epoc  2.339632034301758 loss\n",
      "185 Epoc  2.3395988941192627 loss\n",
      "186 Epoc  2.3395659923553467 loss\n",
      "187 Epoc  2.339533567428589 loss\n",
      "188 Epoc  2.33950138092041 loss\n",
      "189 Epoc  2.3394691944122314 loss\n",
      "190 Epoc  2.339437484741211 loss\n",
      "191 Epoc  2.3394060134887695 loss\n",
      "192 Epoc  2.3393754959106445 loss\n",
      "193 Epoc  2.3393442630767822 loss\n",
      "194 Epoc  2.3393137454986572 loss\n",
      "195 Epoc  2.339284658432007 loss\n",
      "196 Epoc  2.3392558097839355 loss\n",
      "197 Epoc  2.3392276763916016 loss\n",
      "198 Epoc  2.3391995429992676 loss\n",
      "199 Epoc  2.3391714096069336 loss\n",
      "200 Epoc  2.3391430377960205 loss\n",
      "201 Epoc  2.3391153812408447 loss\n",
      "202 Epoc  2.33908748626709 loss\n",
      "203 Epoc  2.339059829711914 loss\n",
      "204 Epoc  2.3390326499938965 loss\n",
      "205 Epoc  2.339005708694458 loss\n",
      "206 Epoc  2.3389792442321777 loss\n",
      "207 Epoc  2.33895206451416 loss\n",
      "208 Epoc  2.338926315307617 loss\n",
      "209 Epoc  2.3389015197753906 loss\n",
      "210 Epoc  2.3388772010803223 loss\n",
      "211 Epoc  2.338852882385254 loss\n",
      "212 Epoc  2.3388288021087646 loss\n",
      "213 Epoc  2.3388051986694336 loss\n",
      "214 Epoc  2.3387815952301025 loss\n",
      "215 Epoc  2.3387584686279297 loss\n",
      "216 Epoc  2.338735580444336 loss\n",
      "217 Epoc  2.338712215423584 loss\n",
      "218 Epoc  2.3386893272399902 loss\n",
      "219 Epoc  2.3386662006378174 loss\n",
      "220 Epoc  2.338643789291382 loss\n",
      "221 Epoc  2.3386213779449463 loss\n",
      "222 Epoc  2.3385989665985107 loss\n",
      "223 Epoc  2.338578939437866 loss\n",
      "224 Epoc  2.3385586738586426 loss\n",
      "225 Epoc  2.3385393619537354 loss\n",
      "226 Epoc  2.338519811630249 loss\n",
      "227 Epoc  2.3385002613067627 loss\n",
      "228 Epoc  2.3384807109832764 loss\n",
      "229 Epoc  2.33846116065979 loss\n",
      "230 Epoc  2.33844256401062 loss\n",
      "231 Epoc  2.338423252105713 loss\n",
      "232 Epoc  2.3384037017822266 loss\n",
      "233 Epoc  2.3383848667144775 loss\n",
      "234 Epoc  2.3383657932281494 loss\n",
      "235 Epoc  2.3383476734161377 loss\n",
      "236 Epoc  2.338329315185547 loss\n",
      "237 Epoc  2.338310956954956 loss\n",
      "238 Epoc  2.3382925987243652 loss\n",
      "239 Epoc  2.3382744789123535 loss\n",
      "240 Epoc  2.338258743286133 loss\n",
      "241 Epoc  2.3382441997528076 loss\n",
      "242 Epoc  2.338228940963745 loss\n",
      "243 Epoc  2.3382139205932617 loss\n",
      "244 Epoc  2.3381996154785156 loss\n",
      "245 Epoc  2.3381855487823486 loss\n",
      "246 Epoc  2.3381714820861816 loss\n",
      "247 Epoc  2.3381571769714355 loss\n",
      "248 Epoc  2.3381426334381104 loss\n",
      "249 Epoc  2.3381288051605225 loss\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 Epoc  2.3381149768829346 loss\n",
      "251 Epoc  2.338101387023926 loss\n",
      "252 Epoc  2.338087797164917 loss\n",
      "253 Epoc  2.338074207305908 loss\n",
      "254 Epoc  2.3380610942840576 loss\n",
      "255 Epoc  2.338047981262207 loss\n",
      "256 Epoc  2.3380351066589355 loss\n",
      "257 Epoc  2.338021755218506 loss\n",
      "258 Epoc  2.3380093574523926 loss\n",
      "259 Epoc  2.3379969596862793 loss\n",
      "260 Epoc  2.337984561920166 loss\n",
      "261 Epoc  2.3379719257354736 loss\n",
      "262 Epoc  2.3379595279693604 loss\n",
      "263 Epoc  2.337946891784668 loss\n",
      "264 Epoc  2.3379344940185547 loss\n",
      "265 Epoc  2.3379218578338623 loss\n",
      "266 Epoc  2.337909460067749 loss\n",
      "267 Epoc  2.3378965854644775 loss\n",
      "268 Epoc  2.337883949279785 loss\n",
      "269 Epoc  2.3378713130950928 loss\n",
      "270 Epoc  2.3378584384918213 loss\n",
      "271 Epoc  2.3378472328186035 loss\n",
      "272 Epoc  2.337836265563965 loss\n",
      "273 Epoc  2.337825298309326 loss\n",
      "274 Epoc  2.3378143310546875 loss\n",
      "275 Epoc  2.337803602218628 loss\n",
      "276 Epoc  2.337792158126831 loss\n",
      "277 Epoc  2.3377814292907715 loss\n",
      "278 Epoc  2.337770462036133 loss\n",
      "279 Epoc  2.3377597332000732 loss\n",
      "280 Epoc  2.3377485275268555 loss\n",
      "281 Epoc  2.337737798690796 loss\n",
      "282 Epoc  2.3377273082733154 loss\n",
      "283 Epoc  2.337716579437256 loss\n",
      "284 Epoc  2.3377060890197754 loss\n",
      "285 Epoc  2.337695837020874 loss\n",
      "286 Epoc  2.337686061859131 loss\n",
      "287 Epoc  2.337676525115967 loss\n",
      "288 Epoc  2.3376667499542236 loss\n",
      "289 Epoc  2.3376564979553223 loss\n",
      "290 Epoc  2.3376471996307373 loss\n",
      "291 Epoc  2.337637186050415 loss\n",
      "292 Epoc  2.337627649307251 loss\n",
      "293 Epoc  2.337618350982666 loss\n",
      "294 Epoc  2.337608575820923 loss\n",
      "295 Epoc  2.3375988006591797 loss\n",
      "296 Epoc  2.3375895023345947 loss\n",
      "297 Epoc  2.3375802040100098 loss\n",
      "298 Epoc  2.3375706672668457 loss\n",
      "299 Epoc  2.3375608921051025 loss\n",
      "300 Epoc  2.3375515937805176 loss\n",
      "301 Epoc  2.3375425338745117 loss\n",
      "302 Epoc  2.3375332355499268 loss\n",
      "303 Epoc  2.337523937225342 loss\n",
      "304 Epoc  2.3375144004821777 loss\n",
      "305 Epoc  2.3375051021575928 loss\n",
      "306 Epoc  2.337495803833008 loss\n",
      "307 Epoc  2.337486505508423 loss\n",
      "308 Epoc  2.337477684020996 loss\n",
      "309 Epoc  2.3374691009521484 loss\n",
      "310 Epoc  2.3374602794647217 loss\n",
      "311 Epoc  2.337451696395874 loss\n",
      "312 Epoc  2.3374433517456055 loss\n",
      "313 Epoc  2.337435483932495 loss\n",
      "314 Epoc  2.337428092956543 loss\n",
      "315 Epoc  2.3374199867248535 loss\n",
      "316 Epoc  2.337412118911743 loss\n",
      "317 Epoc  2.3374054431915283 loss\n",
      "318 Epoc  2.3373985290527344 loss\n",
      "319 Epoc  2.3373913764953613 loss\n",
      "320 Epoc  2.3373847007751465 loss\n",
      "321 Epoc  2.3373782634735107 loss\n",
      "322 Epoc  2.3373711109161377 loss\n",
      "323 Epoc  2.337364912033081 loss\n",
      "324 Epoc  2.337357997894287 loss\n",
      "325 Epoc  2.3373517990112305 loss\n",
      "326 Epoc  2.3373451232910156 loss\n",
      "327 Epoc  2.337339401245117 loss\n",
      "328 Epoc  2.3373332023620605 loss\n",
      "329 Epoc  2.337327003479004 loss\n",
      "330 Epoc  2.3373212814331055 loss\n",
      "331 Epoc  2.3373148441314697 loss\n",
      "332 Epoc  2.337308883666992 loss\n",
      "333 Epoc  2.3373031616210938 loss\n",
      "334 Epoc  2.337296962738037 loss\n",
      "335 Epoc  2.3372907638549805 loss\n",
      "336 Epoc  2.337284803390503 loss\n",
      "337 Epoc  2.3372786045074463 loss\n",
      "338 Epoc  2.337272882461548 loss\n",
      "339 Epoc  2.3372671604156494 loss\n",
      "340 Epoc  2.33726167678833 loss\n",
      "341 Epoc  2.3372573852539062 loss\n",
      "342 Epoc  2.3372528553009033 loss\n",
      "343 Epoc  2.337247610092163 loss\n",
      "344 Epoc  2.337242603302002 loss\n",
      "345 Epoc  2.337237596511841 loss\n",
      "346 Epoc  2.3372323513031006 loss\n",
      "347 Epoc  2.3372271060943604 loss\n",
      "348 Epoc  2.337222099304199 loss\n",
      "349 Epoc  2.337217092514038 loss\n",
      "350 Epoc  2.337212324142456 loss\n",
      "351 Epoc  2.3372080326080322 loss\n",
      "352 Epoc  2.3372037410736084 loss\n",
      "353 Epoc  2.3371987342834473 loss\n",
      "354 Epoc  2.3371946811676025 loss\n",
      "355 Epoc  2.337190866470337 loss\n",
      "356 Epoc  2.337186574935913 loss\n",
      "357 Epoc  2.3371827602386475 loss\n",
      "358 Epoc  2.3371784687042236 loss\n",
      "359 Epoc  2.337174654006958 loss\n",
      "360 Epoc  2.3371706008911133 loss\n",
      "361 Epoc  2.3371670246124268 loss\n",
      "362 Epoc  2.337162971496582 loss\n",
      "363 Epoc  2.3371591567993164 loss\n",
      "364 Epoc  2.337155342102051 loss\n",
      "365 Epoc  2.337151288986206 loss\n",
      "366 Epoc  2.3371474742889404 loss\n",
      "367 Epoc  2.3371434211730957 loss\n",
      "368 Epoc  2.33713960647583 loss\n",
      "369 Epoc  2.3371355533599854 loss\n",
      "370 Epoc  2.3371317386627197 loss\n",
      "371 Epoc  2.337128162384033 loss\n",
      "372 Epoc  2.3371241092681885 loss\n",
      "373 Epoc  2.337120532989502 loss\n",
      "374 Epoc  2.3371164798736572 loss\n",
      "375 Epoc  2.337113380432129 loss\n",
      "376 Epoc  2.337109327316284 loss\n",
      "377 Epoc  2.3371057510375977 loss\n",
      "378 Epoc  2.337102174758911 loss\n",
      "379 Epoc  2.3370981216430664 loss\n",
      "380 Epoc  2.33709454536438 loss\n",
      "381 Epoc  2.3370907306671143 loss\n",
      "382 Epoc  2.337087631225586 loss\n",
      "383 Epoc  2.337083339691162 loss\n",
      "384 Epoc  2.337080478668213 loss\n",
      "385 Epoc  2.3370769023895264 loss\n",
      "386 Epoc  2.33707332611084 loss\n",
      "387 Epoc  2.3370704650878906 loss\n",
      "388 Epoc  2.3370652198791504 loss\n",
      "389 Epoc  2.3370628356933594 loss\n",
      "390 Epoc  2.3370587825775146 loss\n",
      "391 Epoc  2.337054491043091 loss\n",
      "392 Epoc  2.3370516300201416 loss\n",
      "393 Epoc  2.337048292160034 loss\n",
      "394 Epoc  2.337043523788452 loss\n",
      "395 Epoc  2.337040662765503 loss\n",
      "396 Epoc  2.337036609649658 loss\n",
      "397 Epoc  2.3370327949523926 loss\n",
      "398 Epoc  2.337028980255127 loss\n",
      "399 Epoc  2.3370254039764404 loss\n",
      "400 Epoc  2.3370227813720703 loss\n",
      "401 Epoc  2.337017774581909 loss\n",
      "402 Epoc  2.3370139598846436 loss\n",
      "403 Epoc  2.3370113372802734 loss\n",
      "404 Epoc  2.3370065689086914 loss\n",
      "405 Epoc  2.337003707885742 loss\n",
      "406 Epoc  2.3369996547698975 loss\n",
      "407 Epoc  2.336996078491211 loss\n",
      "408 Epoc  2.3369925022125244 loss\n",
      "409 Epoc  2.336989164352417 loss\n",
      "410 Epoc  2.3369860649108887 loss\n",
      "411 Epoc  2.336982488632202 loss\n",
      "412 Epoc  2.3369789123535156 loss\n",
      "413 Epoc  2.3369758129119873 loss\n",
      "414 Epoc  2.3369719982147217 loss\n",
      "415 Epoc  2.3369691371917725 loss\n",
      "416 Epoc  2.336966037750244 loss\n",
      "417 Epoc  2.3369624614715576 loss\n",
      "418 Epoc  2.3369593620300293 loss\n",
      "419 Epoc  2.336956024169922 loss\n",
      "420 Epoc  2.3369526863098145 loss\n",
      "421 Epoc  2.3369500637054443 loss\n",
      "422 Epoc  2.3369460105895996 loss\n",
      "423 Epoc  2.3369431495666504 loss\n",
      "424 Epoc  2.336939573287964 loss\n",
      "425 Epoc  2.3369362354278564 loss\n",
      "426 Epoc  2.3369336128234863 loss\n",
      "427 Epoc  2.3369300365448 loss\n",
      "428 Epoc  2.336928129196167 loss\n",
      "429 Epoc  2.3369245529174805 loss\n",
      "430 Epoc  2.336921215057373 loss\n",
      "431 Epoc  2.336918592453003 loss\n",
      "432 Epoc  2.3369140625 loss\n",
      "433 Epoc  2.33691143989563 loss\n",
      "434 Epoc  2.336909055709839 loss\n",
      "435 Epoc  2.336904764175415 loss\n",
      "436 Epoc  2.336902618408203 loss\n",
      "437 Epoc  2.3368990421295166 loss\n",
      "438 Epoc  2.33689546585083 loss\n",
      "439 Epoc  2.336892604827881 loss\n",
      "440 Epoc  2.3368897438049316 loss\n",
      "441 Epoc  2.336885690689087 loss\n",
      "442 Epoc  2.336883544921875 loss\n",
      "443 Epoc  2.3368797302246094 loss\n",
      "444 Epoc  2.3368773460388184 loss\n",
      "445 Epoc  2.3368756771087646 loss\n",
      "446 Epoc  2.3368706703186035 loss\n",
      "447 Epoc  2.336867332458496 loss\n",
      "448 Epoc  2.3368642330169678 loss\n",
      "449 Epoc  2.3368613719940186 loss\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,450):\n",
    "    y_pred=Neural_Network.forward(X_train)\n",
    "    loss=loss_function(y_pred,y_train)\n",
    "    list_of_loss.append(loss)\n",
    "    print(\"{} Epoc  {} loss\".format(i,loss.item()))\n",
    "    optimizer.zero_grad()  ## Initially our derivative will be 0\n",
    "    loss.backward()        ## Do the first propagation\n",
    "    optimizer.step()        ## Performs a single optimization step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1ece9e3c588>]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(list_of_loss,range(0,450))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.no_grad() tells PyTorch that we do not want to perform back-propagation,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(Neural_Network,\"first_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ANN(\n",
       "  (fc1): Linear(in_features=8, out_features=15, bias=True)\n",
       "  (fc2): Linear(in_features=15, out_features=11, bias=True)\n",
       "  (fc3): Linear(in_features=11, out_features=12, bias=True)\n",
       "  (output): Linear(in_features=12, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load(\"first_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "point1=sc.transform([[0,125,97,30,218.9,106.2,52.2,59.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "point2=torch.FloatTensor(point1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.9712]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(Neural_Network(point2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
