{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"abalone_original.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>length</th>\n",
       "      <th>diameter</th>\n",
       "      <th>height</th>\n",
       "      <th>whole-weight</th>\n",
       "      <th>shucked-weight</th>\n",
       "      <th>viscera-weight</th>\n",
       "      <th>shell-weight</th>\n",
       "      <th>rings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>91</td>\n",
       "      <td>73</td>\n",
       "      <td>19</td>\n",
       "      <td>102.8</td>\n",
       "      <td>44.9</td>\n",
       "      <td>20.2</td>\n",
       "      <td>30.0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>70</td>\n",
       "      <td>53</td>\n",
       "      <td>18</td>\n",
       "      <td>45.1</td>\n",
       "      <td>19.9</td>\n",
       "      <td>9.7</td>\n",
       "      <td>14.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F</td>\n",
       "      <td>106</td>\n",
       "      <td>84</td>\n",
       "      <td>27</td>\n",
       "      <td>135.4</td>\n",
       "      <td>51.3</td>\n",
       "      <td>28.3</td>\n",
       "      <td>42.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>88</td>\n",
       "      <td>73</td>\n",
       "      <td>25</td>\n",
       "      <td>103.2</td>\n",
       "      <td>43.1</td>\n",
       "      <td>22.8</td>\n",
       "      <td>31.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I</td>\n",
       "      <td>66</td>\n",
       "      <td>51</td>\n",
       "      <td>16</td>\n",
       "      <td>41.0</td>\n",
       "      <td>17.9</td>\n",
       "      <td>7.9</td>\n",
       "      <td>11.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4172</th>\n",
       "      <td>F</td>\n",
       "      <td>113</td>\n",
       "      <td>90</td>\n",
       "      <td>33</td>\n",
       "      <td>177.4</td>\n",
       "      <td>74.0</td>\n",
       "      <td>47.8</td>\n",
       "      <td>49.8</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4173</th>\n",
       "      <td>M</td>\n",
       "      <td>118</td>\n",
       "      <td>88</td>\n",
       "      <td>27</td>\n",
       "      <td>193.2</td>\n",
       "      <td>87.8</td>\n",
       "      <td>42.9</td>\n",
       "      <td>52.1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4174</th>\n",
       "      <td>M</td>\n",
       "      <td>120</td>\n",
       "      <td>95</td>\n",
       "      <td>41</td>\n",
       "      <td>235.2</td>\n",
       "      <td>105.1</td>\n",
       "      <td>57.5</td>\n",
       "      <td>61.6</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4175</th>\n",
       "      <td>F</td>\n",
       "      <td>125</td>\n",
       "      <td>97</td>\n",
       "      <td>30</td>\n",
       "      <td>218.9</td>\n",
       "      <td>106.2</td>\n",
       "      <td>52.2</td>\n",
       "      <td>59.2</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4176</th>\n",
       "      <td>M</td>\n",
       "      <td>142</td>\n",
       "      <td>111</td>\n",
       "      <td>39</td>\n",
       "      <td>389.7</td>\n",
       "      <td>189.1</td>\n",
       "      <td>75.3</td>\n",
       "      <td>99.0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4177 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sex  length  diameter  height  whole-weight  shucked-weight  \\\n",
       "0      M      91        73      19         102.8            44.9   \n",
       "1      M      70        53      18          45.1            19.9   \n",
       "2      F     106        84      27         135.4            51.3   \n",
       "3      M      88        73      25         103.2            43.1   \n",
       "4      I      66        51      16          41.0            17.9   \n",
       "...   ..     ...       ...     ...           ...             ...   \n",
       "4172   F     113        90      33         177.4            74.0   \n",
       "4173   M     118        88      27         193.2            87.8   \n",
       "4174   M     120        95      41         235.2           105.1   \n",
       "4175   F     125        97      30         218.9           106.2   \n",
       "4176   M     142       111      39         389.7           189.1   \n",
       "\n",
       "      viscera-weight  shell-weight  rings  \n",
       "0               20.2          30.0     15  \n",
       "1                9.7          14.0      7  \n",
       "2               28.3          42.0      9  \n",
       "3               22.8          31.0     10  \n",
       "4                7.9          11.0      7  \n",
       "...              ...           ...    ...  \n",
       "4172            47.8          49.8     11  \n",
       "4173            42.9          52.1     10  \n",
       "4174            57.5          61.6      9  \n",
       "4175            52.2          59.2     10  \n",
       "4176            75.3          99.0     12  \n",
       "\n",
       "[4177 rows x 9 columns]"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sex']=pd.get_dummies(data['sex'],drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>length</th>\n",
       "      <th>diameter</th>\n",
       "      <th>height</th>\n",
       "      <th>whole-weight</th>\n",
       "      <th>shucked-weight</th>\n",
       "      <th>viscera-weight</th>\n",
       "      <th>shell-weight</th>\n",
       "      <th>rings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>91</td>\n",
       "      <td>73</td>\n",
       "      <td>19</td>\n",
       "      <td>102.8</td>\n",
       "      <td>44.9</td>\n",
       "      <td>20.2</td>\n",
       "      <td>30.0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "      <td>53</td>\n",
       "      <td>18</td>\n",
       "      <td>45.1</td>\n",
       "      <td>19.9</td>\n",
       "      <td>9.7</td>\n",
       "      <td>14.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>106</td>\n",
       "      <td>84</td>\n",
       "      <td>27</td>\n",
       "      <td>135.4</td>\n",
       "      <td>51.3</td>\n",
       "      <td>28.3</td>\n",
       "      <td>42.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>88</td>\n",
       "      <td>73</td>\n",
       "      <td>25</td>\n",
       "      <td>103.2</td>\n",
       "      <td>43.1</td>\n",
       "      <td>22.8</td>\n",
       "      <td>31.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>66</td>\n",
       "      <td>51</td>\n",
       "      <td>16</td>\n",
       "      <td>41.0</td>\n",
       "      <td>17.9</td>\n",
       "      <td>7.9</td>\n",
       "      <td>11.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4172</th>\n",
       "      <td>0</td>\n",
       "      <td>113</td>\n",
       "      <td>90</td>\n",
       "      <td>33</td>\n",
       "      <td>177.4</td>\n",
       "      <td>74.0</td>\n",
       "      <td>47.8</td>\n",
       "      <td>49.8</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4173</th>\n",
       "      <td>0</td>\n",
       "      <td>118</td>\n",
       "      <td>88</td>\n",
       "      <td>27</td>\n",
       "      <td>193.2</td>\n",
       "      <td>87.8</td>\n",
       "      <td>42.9</td>\n",
       "      <td>52.1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4174</th>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>95</td>\n",
       "      <td>41</td>\n",
       "      <td>235.2</td>\n",
       "      <td>105.1</td>\n",
       "      <td>57.5</td>\n",
       "      <td>61.6</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4175</th>\n",
       "      <td>0</td>\n",
       "      <td>125</td>\n",
       "      <td>97</td>\n",
       "      <td>30</td>\n",
       "      <td>218.9</td>\n",
       "      <td>106.2</td>\n",
       "      <td>52.2</td>\n",
       "      <td>59.2</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4176</th>\n",
       "      <td>0</td>\n",
       "      <td>142</td>\n",
       "      <td>111</td>\n",
       "      <td>39</td>\n",
       "      <td>389.7</td>\n",
       "      <td>189.1</td>\n",
       "      <td>75.3</td>\n",
       "      <td>99.0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4177 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sex  length  diameter  height  whole-weight  shucked-weight  \\\n",
       "0       0      91        73      19         102.8            44.9   \n",
       "1       0      70        53      18          45.1            19.9   \n",
       "2       0     106        84      27         135.4            51.3   \n",
       "3       0      88        73      25         103.2            43.1   \n",
       "4       1      66        51      16          41.0            17.9   \n",
       "...   ...     ...       ...     ...           ...             ...   \n",
       "4172    0     113        90      33         177.4            74.0   \n",
       "4173    0     118        88      27         193.2            87.8   \n",
       "4174    0     120        95      41         235.2           105.1   \n",
       "4175    0     125        97      30         218.9           106.2   \n",
       "4176    0     142       111      39         389.7           189.1   \n",
       "\n",
       "      viscera-weight  shell-weight  rings  \n",
       "0               20.2          30.0     15  \n",
       "1                9.7          14.0      7  \n",
       "2               28.3          42.0      9  \n",
       "3               22.8          31.0     10  \n",
       "4                7.9          11.0      7  \n",
       "...              ...           ...    ...  \n",
       "4172            47.8          49.8     11  \n",
       "4173            42.9          52.1     10  \n",
       "4174            57.5          61.6      9  \n",
       "4175            52.2          59.2     10  \n",
       "4176            75.3          99.0     12  \n",
       "\n",
       "[4177 rows x 9 columns]"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=data.iloc[:,0:8]\n",
    "y=data['rings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>length</th>\n",
       "      <th>diameter</th>\n",
       "      <th>height</th>\n",
       "      <th>whole-weight</th>\n",
       "      <th>shucked-weight</th>\n",
       "      <th>viscera-weight</th>\n",
       "      <th>shell-weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>91</td>\n",
       "      <td>73</td>\n",
       "      <td>19</td>\n",
       "      <td>102.8</td>\n",
       "      <td>44.9</td>\n",
       "      <td>20.2</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "      <td>53</td>\n",
       "      <td>18</td>\n",
       "      <td>45.1</td>\n",
       "      <td>19.9</td>\n",
       "      <td>9.7</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>106</td>\n",
       "      <td>84</td>\n",
       "      <td>27</td>\n",
       "      <td>135.4</td>\n",
       "      <td>51.3</td>\n",
       "      <td>28.3</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>88</td>\n",
       "      <td>73</td>\n",
       "      <td>25</td>\n",
       "      <td>103.2</td>\n",
       "      <td>43.1</td>\n",
       "      <td>22.8</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>66</td>\n",
       "      <td>51</td>\n",
       "      <td>16</td>\n",
       "      <td>41.0</td>\n",
       "      <td>17.9</td>\n",
       "      <td>7.9</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4172</th>\n",
       "      <td>0</td>\n",
       "      <td>113</td>\n",
       "      <td>90</td>\n",
       "      <td>33</td>\n",
       "      <td>177.4</td>\n",
       "      <td>74.0</td>\n",
       "      <td>47.8</td>\n",
       "      <td>49.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4173</th>\n",
       "      <td>0</td>\n",
       "      <td>118</td>\n",
       "      <td>88</td>\n",
       "      <td>27</td>\n",
       "      <td>193.2</td>\n",
       "      <td>87.8</td>\n",
       "      <td>42.9</td>\n",
       "      <td>52.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4174</th>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>95</td>\n",
       "      <td>41</td>\n",
       "      <td>235.2</td>\n",
       "      <td>105.1</td>\n",
       "      <td>57.5</td>\n",
       "      <td>61.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4175</th>\n",
       "      <td>0</td>\n",
       "      <td>125</td>\n",
       "      <td>97</td>\n",
       "      <td>30</td>\n",
       "      <td>218.9</td>\n",
       "      <td>106.2</td>\n",
       "      <td>52.2</td>\n",
       "      <td>59.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4176</th>\n",
       "      <td>0</td>\n",
       "      <td>142</td>\n",
       "      <td>111</td>\n",
       "      <td>39</td>\n",
       "      <td>389.7</td>\n",
       "      <td>189.1</td>\n",
       "      <td>75.3</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4177 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sex  length  diameter  height  whole-weight  shucked-weight  \\\n",
       "0       0      91        73      19         102.8            44.9   \n",
       "1       0      70        53      18          45.1            19.9   \n",
       "2       0     106        84      27         135.4            51.3   \n",
       "3       0      88        73      25         103.2            43.1   \n",
       "4       1      66        51      16          41.0            17.9   \n",
       "...   ...     ...       ...     ...           ...             ...   \n",
       "4172    0     113        90      33         177.4            74.0   \n",
       "4173    0     118        88      27         193.2            87.8   \n",
       "4174    0     120        95      41         235.2           105.1   \n",
       "4175    0     125        97      30         218.9           106.2   \n",
       "4176    0     142       111      39         389.7           189.1   \n",
       "\n",
       "      viscera-weight  shell-weight  \n",
       "0               20.2          30.0  \n",
       "1                9.7          14.0  \n",
       "2               28.3          42.0  \n",
       "3               22.8          31.0  \n",
       "4                7.9          11.0  \n",
       "...              ...           ...  \n",
       "4172            47.8          49.8  \n",
       "4173            42.9          52.1  \n",
       "4174            57.5          61.6  \n",
       "4175            52.2          59.2  \n",
       "4176            75.3          99.0  \n",
       "\n",
       "[4177 rows x 8 columns]"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       15\n",
       "1        7\n",
       "2        9\n",
       "3       10\n",
       "4        7\n",
       "        ..\n",
       "4172    11\n",
       "4173    10\n",
       "4174     9\n",
       "4175    10\n",
       "4176    12\n",
       "Name: rings, Length: 4177, dtype: int64"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "668     13\n",
       "1580     8\n",
       "3784    11\n",
       "463      5\n",
       "2615    12\n",
       "        ..\n",
       "575     11\n",
       "3231    12\n",
       "1084     7\n",
       "290     17\n",
       "2713     4\n",
       "Name: rings, Length: 836, dtype: int64"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3141     5\n",
       "3521     3\n",
       "883     15\n",
       "3627    10\n",
       "2106    14\n",
       "        ..\n",
       "1033    10\n",
       "3264    12\n",
       "1653    10\n",
       "2607     9\n",
       "2732     8\n",
       "Name: rings, Length: 3341, dtype: int64"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we have to convert my data into tensors with Float data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=torch.FloatTensor(X_train)\n",
    "X_test=torch.FloatTensor(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.4761, -2.8886, -2.7757, -1.4109, -1.6309, -1.5576, -1.5894, -1.6550])"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=torch.tensor(np.asarray(y_train))\n",
    "y_test=torch.tensor(np.asarray(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5)"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([13,  8, 11,  5, 12, 11,  7,  8,  7,  9,  8,  8, 11,  9,  4,  7,  7, 17,\n",
       "         7,  7,  7,  5,  8,  9, 10, 10,  5, 15, 10, 14,  8,  4,  9, 13,  7,  7,\n",
       "         8,  9,  8, 11, 15, 12, 17, 16, 11, 11,  8, 10, 11,  6, 13,  7, 13, 20,\n",
       "        12,  7,  8, 10,  7,  7,  9,  9, 11,  8,  7, 12, 13, 17,  8, 12,  9, 17,\n",
       "        10,  6, 11,  8,  8,  9,  8,  8,  8,  6,  7, 13, 11,  6,  9, 12,  5,  6,\n",
       "        11, 11,  8,  7, 16,  8, 11,  8, 18, 11, 12, 14, 12, 11,  6,  9,  7,  6,\n",
       "        11, 11, 11, 12, 20, 10, 14, 10, 10, 12,  4, 12,  7, 13,  6,  8, 17, 15,\n",
       "         9, 11,  7, 11,  8,  7,  7, 10, 11,  9, 10,  9,  8, 16, 16, 13,  6,  9,\n",
       "        10,  9,  8,  8,  8,  5,  8, 11,  5,  9,  9,  6,  8,  7, 10, 11, 12,  8,\n",
       "         9, 10,  5, 16,  7,  6, 15,  9, 10,  3,  6, 10, 11, 12,  5,  8,  5, 13,\n",
       "         9, 11,  7,  9, 15,  8, 10,  8,  5,  9, 19,  9,  8,  9, 11,  9,  9,  9,\n",
       "         5, 15, 10, 14, 12,  8, 12, 11,  5, 10, 11, 23,  4, 11, 11, 10,  9,  9,\n",
       "         9,  8,  5, 13, 12, 14,  8, 11, 10, 14, 13, 12, 12, 12,  7,  7,  7, 19,\n",
       "         9, 15,  8,  4,  8,  9, 10, 12,  9,  9,  9,  8, 12,  7, 11, 20,  8,  7,\n",
       "        12, 13, 11, 14,  6,  8,  9, 11,  9,  8,  9, 11, 20,  9, 12,  7,  6,  7,\n",
       "        11,  7,  9,  9,  5,  9,  9,  9, 10, 11,  7, 12,  8, 10, 12, 13,  9, 10,\n",
       "        11, 10,  8,  9,  4,  9,  7,  8,  6,  6,  9, 12, 10,  8, 15, 10, 13, 10,\n",
       "         8, 10, 14,  8,  9, 12,  8,  8, 12,  9, 10,  7,  8, 10, 11, 14, 10, 10,\n",
       "        15,  8,  9,  9, 23, 10,  8,  8,  5,  9, 10, 13,  7,  8, 11, 13,  9, 12,\n",
       "        11, 10, 10,  6,  5, 12, 15,  7,  5, 10, 10, 13, 10, 11, 10,  9,  7, 13,\n",
       "        10, 10,  7,  9, 13,  5,  6,  9,  7, 11, 11,  7, 14,  7, 17, 12, 12,  8,\n",
       "        12,  6,  8, 14, 11, 15,  5, 11, 11,  8,  7,  7, 14,  8, 14, 14, 21,  8,\n",
       "        11,  8, 11, 19,  8,  9,  7, 10,  9, 10,  5, 19, 16,  9, 10,  7, 11, 12,\n",
       "        15, 11, 20, 12, 13, 10,  8, 10, 11,  9,  5, 10, 11, 11, 12, 16,  9, 11,\n",
       "         9, 14, 14, 11, 11,  9, 12, 11,  7,  6, 12,  9,  7,  6, 10,  4, 15, 10,\n",
       "         3,  5,  9,  6,  5, 11, 11,  9,  8, 13,  7, 14,  7,  8, 12, 10, 11, 14,\n",
       "        16,  6, 10, 10,  8, 10,  8, 11,  7,  5,  9, 10, 12,  9,  8, 12, 16, 11,\n",
       "         9, 13,  8, 10,  3,  7, 10, 10,  6,  6,  9,  6, 13,  9,  7, 18,  5, 10,\n",
       "         9,  9,  9, 18,  9,  7,  9,  8, 18, 15, 10,  8, 12, 12, 15,  8,  8,  9,\n",
       "         9, 10,  6, 16, 10, 10,  7,  7,  8,  8,  9,  9,  5,  9, 10, 12, 10, 13,\n",
       "        10, 12,  8, 12,  5,  9,  8,  8, 10, 11, 10, 17, 12, 11,  8,  8,  8,  9,\n",
       "         9, 12, 13, 12, 10,  9, 13,  6, 11,  9, 15,  8, 13,  8,  8,  8,  9,  8,\n",
       "        10,  9, 11,  3,  8,  6,  6,  6,  9,  9, 17, 10,  8, 13,  7,  9, 14,  7,\n",
       "        13, 10,  4, 11,  7, 10,  8, 11, 10, 12, 10,  9,  7, 10, 10, 11,  6, 11,\n",
       "        12,  9, 13, 12, 12,  7, 13, 16, 11, 11,  6,  8, 12,  9,  7, 12, 10, 12,\n",
       "        13,  7, 12,  7, 11, 11,  5, 10,  8,  9, 14, 10, 12,  9,  9, 11,  6, 14,\n",
       "        13, 18,  7,  7, 10,  8,  7,  9,  9,  9, 10, 16, 14,  8, 20, 15, 13,  8,\n",
       "        10,  7,  6,  9, 10, 10, 12, 11,  5,  8,  8,  7,  7, 10, 15,  8, 10, 10,\n",
       "         9,  7,  9, 11,  9, 19, 10, 10,  9, 13,  7,  9, 29, 10,  8,  9,  6, 10,\n",
       "        10, 20,  9,  8, 11,  6, 11, 13,  6,  5,  7,  9,  8,  7, 14,  8,  7,  7,\n",
       "        20,  4, 10,  6, 13,  7, 11,  9, 19,  9,  7, 11, 10, 19,  8, 16,  7, 11,\n",
       "         7,  6, 12,  9, 10,  6, 12,  7,  9,  8,  7, 11,  8,  8,  8, 11, 10, 12,\n",
       "         7,  7,  8, 21,  7,  4,  7,  6, 12, 10,  9,  7,  9,  6,  7, 11, 11, 11,\n",
       "         8, 11,  6, 22, 10,  9, 17, 14,  7,  7, 16,  9,  8, 13, 10, 12,  8, 11,\n",
       "        12,  9, 14,  9, 11,  9,  3, 14, 10,  7,  7,  6,  7,  7,  9,  9,  7,  6,\n",
       "         9,  7, 10, 10, 10, 11, 10, 11, 19,  8,  5,  6, 11,  5,  8,  8,  9, 13,\n",
       "         5,  7,  9, 11, 12,  7, 17,  4])"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(8,15)\n",
    "        self.fc2 = torch.nn.Linear(15,11)\n",
    "        self.output=torch.nn.Linear(11,1)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x=F.relu(self.output(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "Neural_Network=ANN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function=nn.L1Loss()\n",
    "optimizer=torch.optim.Adam(Neural_Network.parameters(),lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_loss=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\harsh\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\torch\\nn\\modules\\loss.py:94: UserWarning: Using a target size (torch.Size([3341])) that is different to the input size (torch.Size([3341, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 0 loss 9.647315979003906\n",
      "Epoch number 1 loss 9.548131942749023\n",
      "Epoch number 2 loss 9.444482803344727\n",
      "Epoch number 3 loss 9.331290245056152\n",
      "Epoch number 4 loss 9.206449508666992\n",
      "Epoch number 5 loss 9.067841529846191\n",
      "Epoch number 6 loss 8.912153244018555\n",
      "Epoch number 7 loss 8.736790657043457\n",
      "Epoch number 8 loss 8.541031837463379\n",
      "Epoch number 9 loss 8.322858810424805\n",
      "Epoch number 10 loss 8.079782485961914\n",
      "Epoch number 11 loss 7.809179306030273\n",
      "Epoch number 12 loss 7.510280609130859\n",
      "Epoch number 13 loss 7.186143398284912\n",
      "Epoch number 14 loss 6.841794490814209\n",
      "Epoch number 15 loss 6.488669395446777\n",
      "Epoch number 16 loss 6.14420223236084\n",
      "Epoch number 17 loss 5.827782154083252\n",
      "Epoch number 18 loss 5.555319786071777\n",
      "Epoch number 19 loss 5.335203170776367\n",
      "Epoch number 20 loss 5.164815425872803\n",
      "Epoch number 21 loss 5.0337700843811035\n",
      "Epoch number 22 loss 4.929916858673096\n",
      "Epoch number 23 loss 4.844832897186279\n",
      "Epoch number 24 loss 4.77339506149292\n",
      "Epoch number 25 loss 4.712378978729248\n",
      "Epoch number 26 loss 4.657106399536133\n",
      "Epoch number 27 loss 4.6022233963012695\n",
      "Epoch number 28 loss 4.5411272048950195\n",
      "Epoch number 29 loss 4.4714531898498535\n",
      "Epoch number 30 loss 4.393730163574219\n",
      "Epoch number 31 loss 4.311840057373047\n",
      "Epoch number 32 loss 4.230937480926514\n",
      "Epoch number 33 loss 4.155986785888672\n",
      "Epoch number 34 loss 4.090280055999756\n",
      "Epoch number 35 loss 4.033874988555908\n",
      "Epoch number 36 loss 3.9857749938964844\n",
      "Epoch number 37 loss 3.942009449005127\n",
      "Epoch number 38 loss 3.898378849029541\n",
      "Epoch number 39 loss 3.8507251739501953\n",
      "Epoch number 40 loss 3.796712875366211\n",
      "Epoch number 41 loss 3.7350478172302246\n",
      "Epoch number 42 loss 3.6654837131500244\n",
      "Epoch number 43 loss 3.5897185802459717\n",
      "Epoch number 44 loss 3.5107293128967285\n",
      "Epoch number 45 loss 3.431414842605591\n",
      "Epoch number 46 loss 3.3549141883850098\n",
      "Epoch number 47 loss 3.2839810848236084\n",
      "Epoch number 48 loss 3.2203855514526367\n",
      "Epoch number 49 loss 3.166290044784546\n",
      "Epoch number 50 loss 3.118960380554199\n",
      "Epoch number 51 loss 3.07509446144104\n",
      "Epoch number 52 loss 3.0314037799835205\n",
      "Epoch number 53 loss 2.9857420921325684\n",
      "Epoch number 54 loss 2.9370415210723877\n",
      "Epoch number 55 loss 2.885876178741455\n",
      "Epoch number 56 loss 2.8336801528930664\n",
      "Epoch number 57 loss 2.7834815979003906\n",
      "Epoch number 58 loss 2.738274097442627\n",
      "Epoch number 59 loss 2.698558807373047\n",
      "Epoch number 60 loss 2.6639599800109863\n",
      "Epoch number 61 loss 2.6337459087371826\n",
      "Epoch number 62 loss 2.6064727306365967\n",
      "Epoch number 63 loss 2.5803866386413574\n",
      "Epoch number 64 loss 2.555344581604004\n",
      "Epoch number 65 loss 2.5309817790985107\n",
      "Epoch number 66 loss 2.5071825981140137\n",
      "Epoch number 67 loss 2.484589099884033\n",
      "Epoch number 68 loss 2.4637715816497803\n",
      "Epoch number 69 loss 2.445467233657837\n",
      "Epoch number 70 loss 2.430655002593994\n",
      "Epoch number 71 loss 2.419016122817993\n",
      "Epoch number 72 loss 2.409992218017578\n",
      "Epoch number 73 loss 2.4028611183166504\n",
      "Epoch number 74 loss 2.3974897861480713\n",
      "Epoch number 75 loss 2.393449306488037\n",
      "Epoch number 76 loss 2.390301465988159\n",
      "Epoch number 77 loss 2.3875792026519775\n",
      "Epoch number 78 loss 2.3853673934936523\n",
      "Epoch number 79 loss 2.384164333343506\n",
      "Epoch number 80 loss 2.384197235107422\n",
      "Epoch number 81 loss 2.3850996494293213\n",
      "Epoch number 82 loss 2.385955810546875\n",
      "Epoch number 83 loss 2.385955333709717\n",
      "Epoch number 84 loss 2.3846914768218994\n",
      "Epoch number 85 loss 2.382389783859253\n",
      "Epoch number 86 loss 2.379626512527466\n",
      "Epoch number 87 loss 2.377052068710327\n",
      "Epoch number 88 loss 2.374966859817505\n",
      "Epoch number 89 loss 2.3731861114501953\n",
      "Epoch number 90 loss 2.37137508392334\n",
      "Epoch number 91 loss 2.3693768978118896\n",
      "Epoch number 92 loss 2.367234706878662\n",
      "Epoch number 93 loss 2.3651444911956787\n",
      "Epoch number 94 loss 2.36342453956604\n",
      "Epoch number 95 loss 2.362090587615967\n",
      "Epoch number 96 loss 2.3610360622406006\n",
      "Epoch number 97 loss 2.3601322174072266\n",
      "Epoch number 98 loss 2.359323024749756\n",
      "Epoch number 99 loss 2.3585970401763916\n",
      "Epoch number 100 loss 2.357919931411743\n",
      "Epoch number 101 loss 2.357309341430664\n",
      "Epoch number 102 loss 2.356750011444092\n",
      "Epoch number 103 loss 2.35622501373291\n",
      "Epoch number 104 loss 2.355708360671997\n",
      "Epoch number 105 loss 2.3551974296569824\n",
      "Epoch number 106 loss 2.354684352874756\n",
      "Epoch number 107 loss 2.354153871536255\n",
      "Epoch number 108 loss 2.353618621826172\n",
      "Epoch number 109 loss 2.3531136512756348\n",
      "Epoch number 110 loss 2.3526480197906494\n",
      "Epoch number 111 loss 2.3522253036499023\n",
      "Epoch number 112 loss 2.351832866668701\n",
      "Epoch number 113 loss 2.351461410522461\n",
      "Epoch number 114 loss 2.3511078357696533\n",
      "Epoch number 115 loss 2.3507776260375977\n",
      "Epoch number 116 loss 2.3504605293273926\n",
      "Epoch number 117 loss 2.350144624710083\n",
      "Epoch number 118 loss 2.3498289585113525\n",
      "Epoch number 119 loss 2.349522829055786\n",
      "Epoch number 120 loss 2.34922456741333\n",
      "Epoch number 121 loss 2.3489291667938232\n",
      "Epoch number 122 loss 2.3486456871032715\n",
      "Epoch number 123 loss 2.3483657836914062\n",
      "Epoch number 124 loss 2.348100423812866\n",
      "Epoch number 125 loss 2.347843647003174\n",
      "Epoch number 126 loss 2.347594976425171\n",
      "Epoch number 127 loss 2.3473520278930664\n",
      "Epoch number 128 loss 2.3471202850341797\n",
      "Epoch number 129 loss 2.3468925952911377\n",
      "Epoch number 130 loss 2.3466718196868896\n",
      "Epoch number 131 loss 2.3464572429656982\n",
      "Epoch number 132 loss 2.346250534057617\n",
      "Epoch number 133 loss 2.3460497856140137\n",
      "Epoch number 134 loss 2.345853567123413\n",
      "Epoch number 135 loss 2.3456625938415527\n",
      "Epoch number 136 loss 2.3454761505126953\n",
      "Epoch number 137 loss 2.3452937602996826\n",
      "Epoch number 138 loss 2.3451151847839355\n",
      "Epoch number 139 loss 2.344942331314087\n",
      "Epoch number 140 loss 2.344775915145874\n",
      "Epoch number 141 loss 2.3446149826049805\n",
      "Epoch number 142 loss 2.3444578647613525\n",
      "Epoch number 143 loss 2.3443055152893066\n",
      "Epoch number 144 loss 2.3441567420959473\n",
      "Epoch number 145 loss 2.344011068344116\n",
      "Epoch number 146 loss 2.3438668251037598\n",
      "Epoch number 147 loss 2.343724250793457\n",
      "Epoch number 148 loss 2.343583106994629\n",
      "Epoch number 149 loss 2.3434488773345947\n",
      "Epoch number 150 loss 2.343318223953247\n",
      "Epoch number 151 loss 2.3431899547576904\n",
      "Epoch number 152 loss 2.343065023422241\n",
      "Epoch number 153 loss 2.3429434299468994\n",
      "Epoch number 154 loss 2.342824697494507\n",
      "Epoch number 155 loss 2.3427083492279053\n",
      "Epoch number 156 loss 2.3425958156585693\n",
      "Epoch number 157 loss 2.342484474182129\n",
      "Epoch number 158 loss 2.3423752784729004\n",
      "Epoch number 159 loss 2.3422672748565674\n",
      "Epoch number 160 loss 2.342161178588867\n",
      "Epoch number 161 loss 2.3420567512512207\n",
      "Epoch number 162 loss 2.34195613861084\n",
      "Epoch number 163 loss 2.3418588638305664\n",
      "Epoch number 164 loss 2.3417630195617676\n",
      "Epoch number 165 loss 2.3416686058044434\n",
      "Epoch number 166 loss 2.3415753841400146\n",
      "Epoch number 167 loss 2.3414859771728516\n",
      "Epoch number 168 loss 2.3413970470428467\n",
      "Epoch number 169 loss 2.341308355331421\n",
      "Epoch number 170 loss 2.3412208557128906\n",
      "Epoch number 171 loss 2.3411407470703125\n",
      "Epoch number 172 loss 2.341064929962158\n",
      "Epoch number 173 loss 2.3409900665283203\n",
      "Epoch number 174 loss 2.340916633605957\n",
      "Epoch number 175 loss 2.340843439102173\n",
      "Epoch number 176 loss 2.3407716751098633\n",
      "Epoch number 177 loss 2.3406996726989746\n",
      "Epoch number 178 loss 2.3406312465667725\n",
      "Epoch number 179 loss 2.340566396713257\n",
      "Epoch number 180 loss 2.340503454208374\n",
      "Epoch number 181 loss 2.3404486179351807\n",
      "Epoch number 182 loss 2.340397834777832\n",
      "Epoch number 183 loss 2.340348958969116\n",
      "Epoch number 184 loss 2.340301036834717\n",
      "Epoch number 185 loss 2.3402559757232666\n",
      "Epoch number 186 loss 2.340212345123291\n",
      "Epoch number 187 loss 2.340172290802002\n",
      "Epoch number 188 loss 2.340134620666504\n",
      "Epoch number 189 loss 2.340097427368164\n",
      "Epoch number 190 loss 2.340061664581299\n",
      "Epoch number 191 loss 2.3400278091430664\n",
      "Epoch number 192 loss 2.3399951457977295\n",
      "Epoch number 193 loss 2.3399641513824463\n",
      "Epoch number 194 loss 2.3399336338043213\n",
      "Epoch number 195 loss 2.3399033546447754\n",
      "Epoch number 196 loss 2.3398733139038086\n",
      "Epoch number 197 loss 2.339844226837158\n",
      "Epoch number 198 loss 2.339815616607666\n",
      "Epoch number 199 loss 2.339787244796753\n",
      "Epoch number 200 loss 2.339759111404419\n",
      "Epoch number 201 loss 2.339731454849243\n",
      "Epoch number 202 loss 2.3397037982940674\n",
      "Epoch number 203 loss 2.339677095413208\n",
      "Epoch number 204 loss 2.3396530151367188\n",
      "Epoch number 205 loss 2.3396308422088623\n",
      "Epoch number 206 loss 2.3396084308624268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 207 loss 2.3395862579345703\n",
      "Epoch number 208 loss 2.339564561843872\n",
      "Epoch number 209 loss 2.339543581008911\n",
      "Epoch number 210 loss 2.3395228385925293\n",
      "Epoch number 211 loss 2.3395028114318848\n",
      "Epoch number 212 loss 2.339482069015503\n",
      "Epoch number 213 loss 2.339461326599121\n",
      "Epoch number 214 loss 2.3394412994384766\n",
      "Epoch number 215 loss 2.339423656463623\n",
      "Epoch number 216 loss 2.3394064903259277\n",
      "Epoch number 217 loss 2.339388847351074\n",
      "Epoch number 218 loss 2.3393712043762207\n",
      "Epoch number 219 loss 2.339353322982788\n",
      "Epoch number 220 loss 2.3393349647521973\n",
      "Epoch number 221 loss 2.3393168449401855\n",
      "Epoch number 222 loss 2.3392984867095947\n",
      "Epoch number 223 loss 2.339280366897583\n",
      "Epoch number 224 loss 2.339263916015625\n",
      "Epoch number 225 loss 2.339247941970825\n",
      "Epoch number 226 loss 2.339231491088867\n",
      "Epoch number 227 loss 2.33921480178833\n",
      "Epoch number 228 loss 2.339197874069214\n",
      "Epoch number 229 loss 2.3391809463500977\n",
      "Epoch number 230 loss 2.3391640186309814\n",
      "Epoch number 231 loss 2.33914852142334\n",
      "Epoch number 232 loss 2.339132785797119\n",
      "Epoch number 233 loss 2.3391170501708984\n",
      "Epoch number 234 loss 2.3391010761260986\n",
      "Epoch number 235 loss 2.339085578918457\n",
      "Epoch number 236 loss 2.339069128036499\n",
      "Epoch number 237 loss 2.3390541076660156\n",
      "Epoch number 238 loss 2.3390390872955322\n",
      "Epoch number 239 loss 2.339024782180786\n",
      "Epoch number 240 loss 2.3390111923217773\n",
      "Epoch number 241 loss 2.3389973640441895\n",
      "Epoch number 242 loss 2.3389832973480225\n",
      "Epoch number 243 loss 2.3389692306518555\n",
      "Epoch number 244 loss 2.338955879211426\n",
      "Epoch number 245 loss 2.3389430046081543\n",
      "Epoch number 246 loss 2.338930130004883\n",
      "Epoch number 247 loss 2.3389170169830322\n",
      "Epoch number 248 loss 2.33890438079834\n",
      "Epoch number 249 loss 2.3388915061950684\n",
      "Epoch number 250 loss 2.338879108428955\n",
      "Epoch number 251 loss 2.3388671875\n",
      "Epoch number 252 loss 2.3388547897338867\n",
      "Epoch number 253 loss 2.3388423919677734\n",
      "Epoch number 254 loss 2.3388302326202393\n",
      "Epoch number 255 loss 2.338818073272705\n",
      "Epoch number 256 loss 2.3388068675994873\n",
      "Epoch number 257 loss 2.3387954235076904\n",
      "Epoch number 258 loss 2.3387839794158936\n",
      "Epoch number 259 loss 2.3387725353240967\n",
      "Epoch number 260 loss 2.338761806488037\n",
      "Epoch number 261 loss 2.3387510776519775\n",
      "Epoch number 262 loss 2.3387398719787598\n",
      "Epoch number 263 loss 2.338728904724121\n",
      "Epoch number 264 loss 2.3387176990509033\n",
      "Epoch number 265 loss 2.338707208633423\n",
      "Epoch number 266 loss 2.338696241378784\n",
      "Epoch number 267 loss 2.3386850357055664\n",
      "Epoch number 268 loss 2.3386735916137695\n",
      "Epoch number 269 loss 2.3386642932891846\n",
      "Epoch number 270 loss 2.338653802871704\n",
      "Epoch number 271 loss 2.3386425971984863\n",
      "Epoch number 272 loss 2.338632822036743\n",
      "Epoch number 273 loss 2.338623285293579\n",
      "Epoch number 274 loss 2.3386127948760986\n",
      "Epoch number 275 loss 2.33860182762146\n",
      "Epoch number 276 loss 2.3385918140411377\n",
      "Epoch number 277 loss 2.338581085205078\n",
      "Epoch number 278 loss 2.338571071624756\n",
      "Epoch number 279 loss 2.3385608196258545\n",
      "Epoch number 280 loss 2.3385510444641113\n",
      "Epoch number 281 loss 2.3385417461395264\n",
      "Epoch number 282 loss 2.338531494140625\n",
      "Epoch number 283 loss 2.3385214805603027\n",
      "Epoch number 284 loss 2.3385117053985596\n",
      "Epoch number 285 loss 2.338502883911133\n",
      "Epoch number 286 loss 2.3384931087493896\n",
      "Epoch number 287 loss 2.3384835720062256\n",
      "Epoch number 288 loss 2.3384745121002197\n",
      "Epoch number 289 loss 2.3384649753570557\n",
      "Epoch number 290 loss 2.3384554386138916\n",
      "Epoch number 291 loss 2.338446617126465\n",
      "Epoch number 292 loss 2.338437795639038\n",
      "Epoch number 293 loss 2.3384289741516113\n",
      "Epoch number 294 loss 2.3384201526641846\n",
      "Epoch number 295 loss 2.338411808013916\n",
      "Epoch number 296 loss 2.338402509689331\n",
      "Epoch number 297 loss 2.3383941650390625\n",
      "Epoch number 298 loss 2.338385581970215\n",
      "Epoch number 299 loss 2.3383772373199463\n",
      "Epoch number 300 loss 2.3383686542510986\n",
      "Epoch number 301 loss 2.33836030960083\n",
      "Epoch number 302 loss 2.3383519649505615\n",
      "Epoch number 303 loss 2.338343381881714\n",
      "Epoch number 304 loss 2.3383352756500244\n",
      "Epoch number 305 loss 2.338327169418335\n",
      "Epoch number 306 loss 2.3383214473724365\n",
      "Epoch number 307 loss 2.3383142948150635\n",
      "Epoch number 308 loss 2.338306188583374\n",
      "Epoch number 309 loss 2.338298797607422\n",
      "Epoch number 310 loss 2.338291883468628\n",
      "Epoch number 311 loss 2.338284730911255\n",
      "Epoch number 312 loss 2.3382773399353027\n",
      "Epoch number 313 loss 2.338270425796509\n",
      "Epoch number 314 loss 2.3382630348205566\n",
      "Epoch number 315 loss 2.3382558822631836\n",
      "Epoch number 316 loss 2.3382482528686523\n",
      "Epoch number 317 loss 2.338240623474121\n",
      "Epoch number 318 loss 2.3382346630096436\n",
      "Epoch number 319 loss 2.338228225708008\n",
      "Epoch number 320 loss 2.3382210731506348\n",
      "Epoch number 321 loss 2.3382139205932617\n",
      "Epoch number 322 loss 2.3382067680358887\n",
      "Epoch number 323 loss 2.338200569152832\n",
      "Epoch number 324 loss 2.338193893432617\n",
      "Epoch number 325 loss 2.3381872177124023\n",
      "Epoch number 326 loss 2.3381803035736084\n",
      "Epoch number 327 loss 2.3381736278533936\n",
      "Epoch number 328 loss 2.3381669521331787\n",
      "Epoch number 329 loss 2.338160991668701\n",
      "Epoch number 330 loss 2.3381543159484863\n",
      "Epoch number 331 loss 2.3381476402282715\n",
      "Epoch number 332 loss 2.338141441345215\n",
      "Epoch number 333 loss 2.338135004043579\n",
      "Epoch number 334 loss 2.3381292819976807\n",
      "Epoch number 335 loss 2.3381223678588867\n",
      "Epoch number 336 loss 2.338116407394409\n",
      "Epoch number 337 loss 2.3381102085113525\n",
      "Epoch number 338 loss 2.338104486465454\n",
      "Epoch number 339 loss 2.3380985260009766\n",
      "Epoch number 340 loss 2.3380918502807617\n",
      "Epoch number 341 loss 2.338085174560547\n",
      "Epoch number 342 loss 2.3380789756774902\n",
      "Epoch number 343 loss 2.3380722999572754\n",
      "Epoch number 344 loss 2.338066816329956\n",
      "Epoch number 345 loss 2.3380608558654785\n",
      "Epoch number 346 loss 2.3380537033081055\n",
      "Epoch number 347 loss 2.3380486965179443\n",
      "Epoch number 348 loss 2.3380422592163086\n",
      "Epoch number 349 loss 2.3380351066589355\n",
      "Epoch number 350 loss 2.338029384613037\n",
      "Epoch number 351 loss 2.3380236625671387\n",
      "Epoch number 352 loss 2.338017463684082\n",
      "Epoch number 353 loss 2.3380110263824463\n",
      "Epoch number 354 loss 2.3380041122436523\n",
      "Epoch number 355 loss 2.3379974365234375\n",
      "Epoch number 356 loss 2.337991714477539\n",
      "Epoch number 357 loss 2.3379852771759033\n",
      "Epoch number 358 loss 2.3379783630371094\n",
      "Epoch number 359 loss 2.337972402572632\n",
      "Epoch number 360 loss 2.337965965270996\n",
      "Epoch number 361 loss 2.3379595279693604\n",
      "Epoch number 362 loss 2.337953567504883\n",
      "Epoch number 363 loss 2.3379476070404053\n",
      "Epoch number 364 loss 2.3379414081573486\n",
      "Epoch number 365 loss 2.3379359245300293\n",
      "Epoch number 366 loss 2.3379299640655518\n",
      "Epoch number 367 loss 2.337923288345337\n",
      "Epoch number 368 loss 2.3379180431365967\n",
      "Epoch number 369 loss 2.337911605834961\n",
      "Epoch number 370 loss 2.3379061222076416\n",
      "Epoch number 371 loss 2.3379006385803223\n",
      "Epoch number 372 loss 2.337894916534424\n",
      "Epoch number 373 loss 2.337888479232788\n",
      "Epoch number 374 loss 2.3378827571868896\n",
      "Epoch number 375 loss 2.3378772735595703\n",
      "Epoch number 376 loss 2.3378713130950928\n",
      "Epoch number 377 loss 2.3378663063049316\n",
      "Epoch number 378 loss 2.3378608226776123\n",
      "Epoch number 379 loss 2.3378548622131348\n",
      "Epoch number 380 loss 2.337850332260132\n",
      "Epoch number 381 loss 2.3378446102142334\n",
      "Epoch number 382 loss 2.337839126586914\n",
      "Epoch number 383 loss 2.337833881378174\n",
      "Epoch number 384 loss 2.3378281593322754\n",
      "Epoch number 385 loss 2.3378231525421143\n",
      "Epoch number 386 loss 2.337817907333374\n",
      "Epoch number 387 loss 2.3378121852874756\n",
      "Epoch number 388 loss 2.3378074169158936\n",
      "Epoch number 389 loss 2.337801933288574\n",
      "Epoch number 390 loss 2.337797164916992\n",
      "Epoch number 391 loss 2.3377914428710938\n",
      "Epoch number 392 loss 2.337785243988037\n",
      "Epoch number 393 loss 2.337779998779297\n",
      "Epoch number 394 loss 2.3377745151519775\n",
      "Epoch number 395 loss 2.3377697467803955\n",
      "Epoch number 396 loss 2.337763547897339\n",
      "Epoch number 397 loss 2.337758779525757\n",
      "Epoch number 398 loss 2.337754011154175\n",
      "Epoch number 399 loss 2.3377492427825928\n",
      "Epoch number 400 loss 2.337745189666748\n",
      "Epoch number 401 loss 2.337740898132324\n",
      "Epoch number 402 loss 2.3377366065979004\n",
      "Epoch number 403 loss 2.3377323150634766\n",
      "Epoch number 404 loss 2.3377280235290527\n",
      "Epoch number 405 loss 2.337723970413208\n",
      "Epoch number 406 loss 2.3377199172973633\n",
      "Epoch number 407 loss 2.3377153873443604\n",
      "Epoch number 408 loss 2.3377113342285156\n",
      "Epoch number 409 loss 2.337707042694092\n",
      "Epoch number 410 loss 2.337703227996826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 411 loss 2.3376991748809814\n",
      "Epoch number 412 loss 2.3376948833465576\n",
      "Epoch number 413 loss 2.337691307067871\n",
      "Epoch number 414 loss 2.3376874923706055\n",
      "Epoch number 415 loss 2.3376834392547607\n",
      "Epoch number 416 loss 2.337679624557495\n",
      "Epoch number 417 loss 2.3376760482788086\n",
      "Epoch number 418 loss 2.337672233581543\n",
      "Epoch number 419 loss 2.3376688957214355\n",
      "Epoch number 420 loss 2.337665319442749\n",
      "Epoch number 421 loss 2.3376612663269043\n",
      "Epoch number 422 loss 2.3376576900482178\n",
      "Epoch number 423 loss 2.3376548290252686\n",
      "Epoch number 424 loss 2.3376500606536865\n",
      "Epoch number 425 loss 2.337646722793579\n",
      "Epoch number 426 loss 2.3376433849334717\n",
      "Epoch number 427 loss 2.337639331817627\n",
      "Epoch number 428 loss 2.3376362323760986\n",
      "Epoch number 429 loss 2.337631940841675\n",
      "Epoch number 430 loss 2.337628126144409\n",
      "Epoch number 431 loss 2.3376245498657227\n",
      "Epoch number 432 loss 2.337620973587036\n",
      "Epoch number 433 loss 2.3376171588897705\n",
      "Epoch number 434 loss 2.337614059448242\n",
      "Epoch number 435 loss 2.3376104831695557\n",
      "Epoch number 436 loss 2.337606906890869\n",
      "Epoch number 437 loss 2.3376033306121826\n",
      "Epoch number 438 loss 2.337599515914917\n",
      "Epoch number 439 loss 2.3375954627990723\n",
      "Epoch number 440 loss 2.3375914096832275\n",
      "Epoch number 441 loss 2.33758807182312\n",
      "Epoch number 442 loss 2.3375842571258545\n",
      "Epoch number 443 loss 2.337580442428589\n",
      "Epoch number 444 loss 2.3375768661499023\n",
      "Epoch number 445 loss 2.3375725746154785\n",
      "Epoch number 446 loss 2.337568759918213\n",
      "Epoch number 447 loss 2.3375649452209473\n",
      "Epoch number 448 loss 2.3375613689422607\n",
      "Epoch number 449 loss 2.337557315826416\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,450):\n",
    "    y_pred=Neural_Network.forward(X_train)\n",
    "    loss=loss_function(y_pred,y_train)\n",
    "    list_of_loss.append(loss)\n",
    "    print(\"Epoch number {} loss {}\".format(i,loss.item()))\n",
    "    optimizer.zero_grad()  ## Initially our derivative will be 0\n",
    "    loss.backward()        ## Do the first propagation\n",
    "    optimizer.step()        ## Performs a single optimization step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x224842130f0>]"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD4CAYAAADvsV2wAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAAZQklEQVR4nO3dfYxd9X3n8ff3PszceR57PDa2x/ZYsQUFEhtwWJpsoixsKkhRoFUSUW1bFKFSdeku2VTq0mqlqtL+keyukibSbiQEad3dNAlLoKAoTUA8tBupAWxjHoyhGLCNx0/DjOfB83jvzHf/OL958Hjsc8ee63PP3M9Lujrn/M65935ty5/fub/zu+eauyMiIitfJukCRETkylDgi4jUCAW+iEiNUOCLiNQIBb6ISI3IJV0AwJo1a7y7uzvpMkREUmXv3r0fuXtnucdXReB3d3ezZ8+epMsQEUkVMzuylOM1pCMiUiMU+CIiNUKBLyJSIxT4IiI1QoEvIlIjFPgiIjVCgS8iUiNSHfivHO7nW8+8w2RpOulSRESqXqoDf9+RM3z3+UOUphX4IiJxUh34IiJSPgW+iEiNUOCLiNQIBb6ISI1YEYGv32EXEYmX6sA3S7oCEZH0SHXgi4hI+RT4IiI1QoEvIlIjFPgiIjViRQS+JumIiMRLdeAbmqYjIlKuVAe+iIiUT4EvIlIjFPgiIjVCgS8iUiNWROC7bqYjIhIr1YGve+mIiJSv7MA3s6yZvWpmPw3bW83sJTM7ZGY/NrO60F4ftg+F/d0Vql1ERJZgKWf4DwIH521/E/i2u28DzgD3hfb7gDOh/dvhOBERSVhZgW9mXcBvAo+EbQNuBR4Ph+wG7g7rd4Vtwv7bwvEiIpKgcs/w/wr4U2A6bHcAA+5eCtvHgI1hfSPwIUDYPxiOP4eZ3W9me8xsT29v76VVLyIiZYsNfDO7Ezjt7nuX843d/WF33+Xuuzo7Oy/vtZapJhGRlSxXxjGfBr5oZl8ACkAr8B2g3cxy4Sy+C+gJx/cAm4BjZpYD2oC+Za9cRESWJPYM393/zN273L0buAd43t3/HfAC8KVw2L3AU2H96bBN2P+8a6K8iEjiLmce/n8Gvm5mh4jG6B8N7Y8CHaH968BDl1eiiIgsh3KGdGa5+4vAi2H9feDmRY4ZB768DLWJiMgySvU3bUVEpHwrIvB1hUBEJF6qA1/f5xIRKV+qA19ERMqnwBcRqREKfBGRGrEyAl8XbUVEYqU68HXJVkSkfKkOfBERKZ8CX0SkRijwRURqhAJfRKRGrIjAd03TERGJlerA150VRETKl+rAFxGR8inwRURqhAJfRKRGKPBFRGrEigh8/QCKiEi8VAe+JumIiJQv1YEvIiLlU+CLiNQIBb6ISI1Q4IuI1IgVEfiapCMiEi/VgW+6mY6ISNlSHfgiIlI+Bb6ISI1Q4IuI1AgFvohIjVgRge+6mY6ISKxUB74m6YiIlC/VgS8iIuVT4IuI1AgFvohIjYgNfDMrmNnLZvaamR0ws78M7VvN7CUzO2RmPzazutBeH7YPhf3dFf4ziIhIGco5w58AbnX3HcBO4HYzuwX4JvBtd98GnAHuC8ffB5wJ7d8Ox1WU5uiIiMSLDXyPnA2b+fBw4Fbg8dC+G7g7rN8Vtgn7b7MK3fRGk3RERMpX1hi+mWXNbD9wGngWeA8YcPdSOOQYsDGsbwQ+BAj7B4GORV7zfjPbY2Z7ent7L+sPISIi8coKfHefcvedQBdwM3DN5b6xuz/s7rvcfVdnZ+flvpyIiMRY0iwddx8AXgB+HWg3s1zY1QX0hPUeYBNA2N8G9C1HsSIicunKmaXTaWbtYb0B+DxwkCj4vxQOuxd4Kqw/HbYJ+5933ftARCRxufhDWA/sNrMsUQfxmLv/1MzeAn5kZv8VeBV4NBz/KPC/zewQ0A/cU4G6z6HuREQkXmzgu/vrwA2LtL9PNJ6/sH0c+PKyVBdHN9MRESmbvmkrIlIjFPgiIjVCgS8iUiMU+CIiNWJFBL7rbjoiIrFSHfiaoyMiUr5UB76IiJRPgS8iUiMU+CIiNUKBLyJSI1ZG4GuSjohIrFQHvm6lIyJSvlQHvoiIlE+BLyJSIxT4IiI1QoEvIlIjVkTga5KOiEi8VAe+6W46IiJlS3Xgi4hI+RT4IiI1QoEvIlIjFPgiIjViRQS+a5qOiEisVAf+zL109BOHIiLx0h34YakzfBGReOkOfE3DFxEpW6oDf4ZO8EVE4qU68Ge+aesa0xERiZXqwNedFUREypfuwA90gi8iEi/Vga8TfBGR8qU78DVNR0SkbKkO/Bka0hERiZfqwJ/94pUmZoqIxEp34GtER0SkbLGBb2abzOwFM3vLzA6Y2YOhfbWZPWtm74blqtBuZvZdMztkZq+b2Y2V/kNoSEdEJF45Z/gl4E/c/VrgFuABM7sWeAh4zt23A8+FbYA7gO3hcT/wvWWvOpi7eZqIiMSJDXx3P+Hu+8L6MHAQ2AjcBewOh+0G7g7rdwF/65FfAe1mtn65Cwf9pq2IyFIsaQzfzLqBG4CXgHXufiLsOgmsC+sbgQ/nPe1YaFv4Wveb2R4z29Pb27vUus+hWyuIiMQrO/DNrBn4CfA1dx+av8+jxF1S6rr7w+6+y913dXZ2LuWp82oKr3VJzxYRqS1lBb6Z5YnC/gfu/kRoPjUzVBOWp0N7D7Bp3tO7QpuIiCSonFk6BjwKHHT3b83b9TRwb1i/F3hqXvvvh9k6twCD84Z+KkIjOiIi8XJlHPNp4PeAN8xsf2j7c+AbwGNmdh9wBPhK2Pcz4AvAIWAU+OpyFjzf3K0VlPgiInFiA9/df8mF71N22yLHO/DAZdZVFs3REREpX6q/aTtDQzoiIvFSHfiapSMiUr50B/7sTxwmXIiISAqkO/A1iC8iUrZ0B35YTusUX0QkVroDP5ziK/BFROKlOvCzmRD40wkXIiKSAikP/Gg5pTN8EZFYqQ78jIZ0RETKtjICf1qBLyISJ9WBPzOGP6XAFxGJlerAnznD1xi+iEi8VAe+ZumIiJQv5YEfLXWGLyISL9WBr4u2IiLlWxmBrzN8EZFYqQ58zdIRESlfqgNfZ/giIuVLdeDPneEnXIiISAqkPPCjpWbpiIjES3Xga5aOiEj5Uh34s1+80hm+iEisFRH4pSkFvohInFQHfl0uKn9SV21FRGKlO/DDVdvJkgJfRCROqgM/HwK/qDN8EZFYqQ782SEdneGLiMRKdeDnwkVbneGLiMRLdeCbGXW5DJOapSMiEivVgQ/RhVsN6YiIxEt/4OcyGtIRESlD6gM/nzWd4YuIlCH1gR+N4SvwRUTipD7wG/M5RiZKSZchIlL1Uh/4TfVZRiYV+CIicWID38y+b2anzezNeW2rzexZM3s3LFeFdjOz75rZITN73cxurGTxAE31Oc5OTFX6bUREUq+cM/y/AW5f0PYQ8Jy7bweeC9sAdwDbw+N+4HvLU+aFNddrSEdEpByxge/u/wT0L2i+C9gd1ncDd89r/1uP/ApoN7P1y1Troprqc4wq8EVEYl3qGP46dz8R1k8C68L6RuDDeccdC23nMbP7zWyPme3p7e29xDKiM/zhcQW+iEicy75o6+4OLPneBu7+sLvvcvddnZ2dl/z+69sKDE+UGBwrXvJriIjUgksN/FMzQzVheTq09wCb5h3XFdoqZuuaJgAOfzRSybcREUm9Sw38p4F7w/q9wFPz2n8/zNa5BRicN/RTETOB/4ECX0TkonJxB5jZD4HPAWvM7BjwF8A3gMfM7D7gCPCVcPjPgC8Ah4BR4KsVqPkcmzsayRi833u20m8lIpJqsYHv7r9zgV23LXKsAw9cblFLUZ/Lsn1tC6/3DF7JtxURSZ3Uf9MW4PqNbRw8MZR0GSIiVW1FBH5LIcfopL5tKyJyMSsi8Otz+hEUEZE4KyLw2xrzTJSmGRrXXHwRkQtZEYG/s6sdgH9+ry/ZQkREqtiKCPxPbl1NR1Mdj/y/95ma1g+ai4gsZkUEfj6b4aE7ruGVw2f42o/3M6yhHRGR88TOw0+LL+/aRO/ZCf77L97hlQ/6+eNbt/Glm7oo5LNJlyYiUhVWxBn+jH//uW088UefYl1bgf/y92/ymf/2At978T3OjEwmXZqISOIs+nJssnbt2uV79uxZttdzd/75vT7+14vv8ctDH1GXy3DnJ9bzu7ds4YZN7ZjZsr2XiEhSzGyvu+8q9/gVM6Qzn5nxqW1r+NS2NRw8McQPXjrCk/t6eGJfD9dtaOW3btjI7ddfRdeqxqRLFRG5YlbkGf5izk6UePLVHn708lEOHI9uw3DdhlZu3rqam7as4qYtq1jf1lDRGkREltNSz/BrJvDnO9I3wj+8eZIX3j7Na8cGGC9G39Ld0Fbgxi2ruHFz1AFcu6GVfHZFXeYQkRVEgb9Exalp3j4xzN4j/ew9OsC+I2foGRgDoJDP8Imu9ugTwOZV3LhlFaub6hKpU0RkIQX+Mjg5OM6+o2fYeyR6HDg+SHEq+nvauqZp9hPATVtWsX1tM5mMLgKLyJWnwK+A8eIUb/QMznYA+46coS9M9Wwp5Lhh8ypu3NzOJ7rauOaqVta3FTQTSEQqTrN0KqCQz/LJ7tV8sns1EE37PNo/OtsB7D1yhu889y4zfWdrIcc161u55qoWPtbZzOaORro7muha1aBrAiKSGAX+JTAztnQ0saWjid++sQuA4fEi75wc5uDJYd4+McTbJ4d5Yl8PZydKs8/LZoyN7Q1s6WhkS+gEtnQ00d3RyKbVjfpWsIhUlAJ/mbQU8uzqXs2u8CkAok8CvWcnONo3yuG+UY70jcwun95/nKHxuc7ADNa3FuheM9cJbOloYuuaJjavbqShTp2BiFweBX4FmRlrWwqsbSmc0xHMGBid5HDfKIc/GuFI3yiH+0Y43DfCLw6cpH/B7SA2tBW4fmMbOza1s3NTOx/vaqO1kL9SfxQRWQEU+Alqb6xjZ2MdOze1n7dvcKzI0b5RPugb4chHIxzqPcsbxwZ55q1TQPSJ4GOdzezoamfnpjZ2blrF1Ve1UJfTNQIRWZwCv0q1NeT5eFcbH+9qO6d9YHSS148N8tqHA+z/cIAX3znNT/YdA6Aul+G6Da3s6Grnhs3tXLehje6ORnK6UCwiaFpm6rk7x86M8dqxgdlO4I2ewdlvD9flMmzrbObqq1q4embW0OpGXRcQWQE0LbPGmBmbVkezfO78xAYASlPTvHNqmLdPDPPOqWHeOTnMr97v48lXe855bmdL/Wz4bwrLmcfalnp9oUxkhVHgr0C5bIbrNrRx3YZzh4MGR4sc7hvhSP8oH/aPcrRvlKP9o7z8QT9P7e9h/q9DFvIZtqxuontNNH20e00T3R1NfGxtE2tbClf4TyQiy0GBX0PaGvPsaGxnxyIXiSdL0/QMjHG0f5SjfXOzhg6dPssLb/cyOTU9e+ya5np+bX0L125o5dr10WPrmiZdKxCpcgp8AaKx/q1ronn/0HnOvqlp5/jAGIf7Rnj31FneOjHEwRND/PUvD892BPW5DNvXNbOts5lta6PHxzqb2dLRpJlDIlVCgS+xspm56wSf2T7XGRSnpnmv9ywHTwzx1vHo28Uvf9DP3+8/fs5zr2otsL6twPr2Bja0FbiqrcD6tgY2tEfrqxrrdMsJkStAgS+XLJ/NcM1VrVxzVSu/dcNc+8hEifd7RzjUO8x7p0foGRjj+MAYrx8b4BcHxpksTZ/3Ws31Odoa8rQ3Ro+2hjxtDXXRdsNMW7TdUsjRWJejsS5LQ12WxnxWw0kiZVDgy7Jrqs8t+h0CiKaR9o9McmJwnOMDY5waGufMaJGB0SIDY5MMjhYZGCtycnCYwbGovTQdP3W4LpuJwn+mE6jL0pjPnd9Wl6MhP7OepSF0HIV8hvpclvpcWOYzc+u5TNjOktXMJUkxBb5cUWZGR3M9Hc31XL/x/A5hIXdndHKKgbEiA6NRhzA0XmR0corRySnGwnK0WGJsZrs4015iYHSS4wPh2GLUNvMdhUuRy1joAGY6h4t3ELPHhOMLS3reufvrchl1OHJZFPhS1cyMpvocTfU5NrYvz28OT097CP+ZDqLERHGaidI0E6Wpc9dL00wUw/KC++fWx4pTDIxNnnfMeHiNy/2eYz5rsx3CbOcxrxMpLOxY8hkKoeMozHYg536iWfg6s/vyc/vqshn9xsMKoMCXmpPJzHUiV5K7U5zyuY5iYWcS07GMF6eYLE0zHjqY8eL5HcrA6OTsa8y0jRenGC9OUcbI2AWZMftJY/HOIupQCvnoMTOM1rBgfWZoLTpmbnitMG+pTzGVo8AXuULMjLqcUZfL0JLA+5emphmf17GML1gu2jZvebHnjhenGRgtMlacYnx2+Czat1T1uczsxfhC6CCa6nI0h066qT5Hc302LOfaWmbXs7PtzfU56nP6dDJDgS9SI3LZDM3ZDM1X8JPNzPDZWLiuMn8obaxYYmxyOlxXmbvOsvC40ckSI5NTnBwa5+xEiZGJEmcnyr8Wk80YTXVZWhvytBaiGWCtDbl56xdpK+RX1D2nKvIvb2a3A98BssAj7v6NSryPiFS3Sg6flaamGZmcYmReJzAyMTXbKYxMluY6iPESw+MlhsaLDI4VOfzR6Oz66OTURd+nkM/Q0VTPqqY8q5vqWd0Ylk15VjXV0dFUx6rGOjqa6+hsKdBayFXtJ4pl/1cwsyzwP4HPA8eAV8zsaXd/a7nfS0RqVy6boa0hQ1vD5f0QUHFqmqGxIkPjJYbGok5gpjMYHCtyZmSS/pEi/SMT9I8W+eCjs/SfnWTkAh1FIZ9hXWuBdS0FOlvrWddSYF1rPetaC6xtqWdta7TdXH/lO4ZKnOHfDBxy9/cBzOxHwF2AAl9Eqk4+m5mdKrwU48UpzoxO0j8SPfrOTtI7PMGpoXFODU9wemict44P8cLQ6UU/RTTWZVnbUs/Xf+Nqvrhjw3L9cS6qEoG/Efhw3vYx4F8tPMjM7gfuB9i8eXMFyhARqZxCPsv6tgbWt8VPFz47UYo6gqHxuU5haILTwxOsbqy7AtVGErto6+4PAw9D9AMoSdUhIlJpzfU5mjujGwomqRI3IOkBNs3b7gptIiKSoEoE/ivAdjPbamZ1wD3A0xV4HxERWYJlH9Jx95KZ/THwC6Jpmd939wPL/T4iIrI0FRnDd/efAT+rxGuLiMil0U3ERURqhAJfRKRGKPBFRGqEAl9EpEaYX+4vMixHEWa9wAjwUdK1lGENqnO5pKFGSEedaagRVOdyWgM0uXtnuU+oisAHMLM97r4r6TriqM7lk4YaIR11pqFGUJ3L6VJq1JCOiEiNUOCLiNSIagr8h5MuoEyqc/mkoUZIR51pqBFU53Jaco1VM4YvIiKVVU1n+CIiUkEKfBGRGpF44JvZJjN7wczeMrMDZvZg0jUtZGYFM3vZzF4LNf5l0jVdjJllzexVM/tp0rVciJkdNrM3zGy/me1Jup7FmFm7mT1uZm+b2UEz+/Wka1rIzK4Of4czjyEz+1rSdS3GzP5T+P/zppn90MwKSde0kJk9GOo7UE1/j2b2fTM7bWZvzmtbbWbPmtm7Ybkq7nUSD3ygBPyJu18L3AI8YGbXJlzTQhPAre6+A9gJ3G5mtyRb0kU9CBxMuogy/Bt331nF852/A/zc3a8BdlCFf6fu/k74O9wJ3ASMAk8mW9X5zGwj8B+BXe5+PdGt0+9Jtqpzmdn1wB8Q/S73DuBOM9uWbFWz/ga4fUHbQ8Bz7r4deC5sX1Tige/uJ9x9X1gfJvpPtTHZqs7lkbNhMx8eVXm128y6gN8EHkm6ljQzszbgs8CjAO4+6e4DiRYV7zbgPXc/knQhF5ADGswsBzQCxxOuZ6FfA15y91F3LwH/CPx2wjUB4O7/BPQvaL4L2B3WdwN3x71O4oE/n5l1AzcALyVcynnCMMl+4DTwrLtXXY3BXwF/CkwnXEccB54xs73hB+2rzVagF/jrMDz2iJk1JV1UjHuAHyZdxGLcvQf4H8BR4AQw6O7PJFvVed4EPmNmHWbWCHyBc3+utdqsc/cTYf0ksC7uCVUT+GbWDPwE+Jq7DyVdz0LuPhU+NncBN4ePf1XFzO4ETrv73qRrKcO/dvcbgTuIhvE+m3RBC+SAG4HvufsNRPd6iv3InJTwc6JfBP5v0rUsJowv30XUkW4Amszsd5Ot6lzufhD4JvAM8HNgPzCVZE3l8mh+feyoQ1UEvpnlicL+B+7+RNL1XEz4WP8C54+nVYNPA180s8PAj4Bbzez/JFvS4sIZH+5+mmjM+eZkKzrPMeDYvE9yjxN1ANXqDmCfu59KupAL+LfAB+7e6+5F4AngUwnXdB53f9Tdb3L3zwJngH9JuqaLOGVm6wHC8nTcExIPfDMzonHSg+7+raTrWYyZdZpZe1hvAD4PvJ1oUYtw9z9z9y537yb6eP+8u1fVWRSAmTWZWcvMOvAbRB+nq4a7nwQ+NLOrQ9NtwFsJlhTnd6jS4ZzgKHCLmTWG//O3UYUXwc1sbVhuJhq//7tkK7qop4F7w/q9wFNxT6jIb9ou0aeB3wPeCGPkAH8efhe3WqwHdptZlqiTfMzdq3bKYwqsA56M/t+TA/7O3X+ebEmL+g/AD8JwyfvAVxOuZ1Gh0/w88IdJ13Ih7v6SmT0O7COamfcq1Xn7gp+YWQdQBB6olgv1ZvZD4HPAGjM7BvwF8A3gMTO7DzgCfCX2dXRrBRGR2pD4kI6IiFwZCnwRkRqhwBcRqREKfBGRGqHAFxGpEQp8EZEaocAXEakR/x8eGcYZQddpJwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(list_of_loss,range(0,450))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.no_grad() tells PyTorch that we do not want to perform back-propagation,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(Neural_Network,\"first_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ANN(\n",
       "  (fc1): Linear(in_features=8, out_features=15, bias=True)\n",
       "  (fc2): Linear(in_features=15, out_features=11, bias=True)\n",
       "  (output): Linear(in_features=11, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load(\"first_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=torch.tensor([1.4761, -2.8886, -2.7757, -1.4109, -1.6309, -1.5576, -1.5894, -1.6550])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.4761, -2.8886, -2.7757, -1.4109, -1.6309, -1.5576, -1.5894, -1.6550])"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9.8750])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(Neural_Network(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
